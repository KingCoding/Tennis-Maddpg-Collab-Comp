{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install .\n",
    "!pip install python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/student/.local/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /home/student/.local/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /home/student/.local/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/student/.local/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/student/.local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/student/.local/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/student/.local/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/student/.local/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/student/.local/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/student/.local/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/student/.local/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/student/.local/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/student/.local/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/student/.local/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (68.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: protobuf==3.20.1 in /home/student/.local/lib/python3.11/site-packages (3.20.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.1 #protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "Restart the Kernel, and verify the protobuf version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.20.1\n"
     ]
    }
   ],
   "source": [
    "import google.protobuf\n",
    "print(google.protobuf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Instructions\n",
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /data/Tennis_Linux_NoVis/Tennis.x86_64\n",
      "Mono path[0] = '/data/Tennis_Linux_NoVis/Tennis_Data/Managed'\n",
      "Mono config path = '/data/Tennis_Linux_NoVis/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/student/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munityagents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnityEnvironment\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mUnityEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/Tennis_Linux_NoVis/Tennis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unityagents/environment.py:64\u001b[0m, in \u001b[0;36mUnityEnvironment.__init__\u001b[0;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m     60\u001b[0m rl_init_parameters_in \u001b[38;5;241m=\u001b[39m UnityRLInitializationInput(\n\u001b[1;32m     61\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     aca_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_academy_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrl_init_parameters_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnityTimeOutException:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unityagents/environment.py:505\u001b[0m, in \u001b[0;36mUnityEnvironment.send_academy_parameters\u001b[0;34m(self, init_parameters)\u001b[0m\n\u001b[1;32m    503\u001b[0m inputs \u001b[38;5;241m=\u001b[39m UnityInput()\n\u001b[1;32m    504\u001b[0m inputs\u001b[38;5;241m.\u001b[39mrl_initialization_input\u001b[38;5;241m.\u001b[39mCopyFrom(init_parameters)\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrl_initialization_output\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unityagents/rpc_communicator.py:59\u001b[0m, in \u001b[0;36mRpcCommunicator.initialize\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityTimeOutException(\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt start socket communication because worker number \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is still in use. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to manually close a previously opened environment \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor use a different worker number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_id)))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mpoll(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityTimeOutException(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Unity environment took too long to respond. Make sure that :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m The environment does not need user interaction to launch\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m The Academy and the External Brain(s) are attached to objects in the Scene\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m The environment and the Python interface have compatible versions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m aca_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\u001b[38;5;241m.\u001b[39munity_output\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions."
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        \n",
    "        print('The state for the first agent looks like:', states[0])\n",
    "        print('The state for the second agent looks like:', states[1])\n",
    "        \n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_in_dim, hidden_out_dim, output_dim, actor=False):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.actor = actor\n",
    "        \n",
    "        if actor:\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_in_dim)\n",
    "            self.fc2 = nn.Linear(hidden_in_dim,hidden_out_dim)\n",
    "            self.fc3 = nn.Linear(hidden_out_dim,output_dim)\n",
    "            self.activation = f.relu #leaky_relu\n",
    "            self.batch_norm_input = nn.BatchNorm1d(hidden_in_dim)\n",
    "            self.batch_norm_hidden1 = nn.BatchNorm1d(hidden_out_dim)\n",
    "        else:\n",
    "            \n",
    "            self.fc1 = nn.Linear(input_dim, hidden_in_dim)            \n",
    "            self.fc2 = nn.Linear(hidden_in_dim + 2, hidden_out_dim)\n",
    "            self.fc3 = nn.Linear(hidden_out_dim, output_dim)\n",
    "            self.activation = f.relu #leaky_relu\n",
    "            self.batch_norm_input = nn.BatchNorm1d(hidden_in_dim)\n",
    "            self.batch_norm_hidden1 = nn.BatchNorm1d(hidden_out_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        if self.actor:\n",
    "            self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "            self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "            self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        else:\n",
    "            self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "            self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "            self.fc3.weight.data.uniform_(-1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.actor:\n",
    "            \n",
    "            h1 = self.activation(self.batch_norm_input(self.fc1(x)))\n",
    "            h2 = self.activation(self.batch_norm_hidden1(self.fc2(h1)))\n",
    "            h3 = self.fc3(h2)\n",
    "            return f.tanh(h3)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            h1 = self.activation(self.batch_norm_input(self.fc1(x[0])))            \n",
    "            h1 = torch.cat((h1,x[1]), dim=1)\n",
    "            h2 = self.activation(self.batch_norm_hidden1(self.fc2(h1)))\n",
    "            h3 = self.fc3(h2)\n",
    "            \n",
    "            return h3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# from https://github.com/songrotek/DDPG/blob/master/ou_noise.py\n",
    "class OUNoise:\n",
    "\n",
    "    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.scale = scale\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state * self.scale\n",
    "\"\"\"\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    \"\"\"\n",
    "    Copy network parameters from source to target\n",
    "    Inputs:\n",
    "        target (torch.nn.Module): Net to copy parameters to\n",
    "        source (torch.nn.Module): Net whose parameters to copy\n",
    "    \"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    \n",
    "    def __init__(self, in_actor, hidden_in_actor, hidden_out_actor, out_actor, in_critic, hidden_in_critic, hidden_out_critic, lr_actor=1.0e-3, lr_critic=1.0e-3):# lr_actor=1.0e-2, lr_critic=1.0e-2):     \n",
    "       \n",
    "        super(DDPGAgent, self).__init__()\n",
    "         \n",
    "        self.actor = Network(in_actor, hidden_in_actor, hidden_out_actor, out_actor, actor=True).to(device)\n",
    "        self.critic = Network(in_critic, hidden_in_critic, hidden_out_critic, 1).to(device)\n",
    "        self.target_actor = Network(in_actor, hidden_in_actor, hidden_out_actor, out_actor, actor=True).to(device)\n",
    "        self.target_critic = Network(in_critic, hidden_in_critic, hidden_out_critic, 1).to(device)\n",
    "        \n",
    "        #self.noise = OUNoise(out_actor, scale=1.0 )\n",
    "        \n",
    "        self.noise = OUNoise(out_actor, 42) #self.noise = OUNoise(out_actor)#, 42) #seed)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        \n",
    "        # initialize targets same as original networks\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "        \n",
    "        #Create Actor and Critic optimizers\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=lr_critic, weight_decay=0)#1.e-5)\n",
    "\n",
    "\n",
    "    def act(self, obs, noise=0.0):\n",
    "        \n",
    "        if torch.is_tensor(obs):\n",
    "            obs = torch.unsqueeze(obs,0).to(device)\n",
    "            \n",
    "        else:\n",
    "            obs = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(obs).cpu().squeeze(0).data.numpy()\n",
    "        self.actor.train()\n",
    "        \n",
    "        #if add_noise:            \n",
    "        action += self.noise.sample() * self.epsilon #self.noise.noise() * self.epsilon\n",
    "        return np.clip(action, -1, 1) \n",
    "\n",
    "\n",
    "\n",
    "    def target_act(self, obs, noise=0.0):\n",
    "        \n",
    "        if torch.is_tensor(obs):\n",
    "            obs = torch.unsqueeze(obs,0).to(device)\n",
    "            \n",
    "        else:\n",
    "            obs = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.target_actor(obs).cpu().squeeze(0).data.numpy()            \n",
    "        self.target_actor.train()\n",
    "        \n",
    "        #if add_noise:            \n",
    "        action += self.noise.sample() * self.epsilon #self.noise.noise() * self.epsilon\n",
    "        \n",
    "        return torch.tensor(np.clip(action, -1, 1))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main code that contains the neural network setup\n",
    "# policy + critic updates\n",
    "# see ddpg.py for other details in the network\n",
    "\n",
    "#import random\n",
    "#import torch\n",
    "#from utilities import soft_update, transpose_to_tensor, transpose_list\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "def transpose_list(mylist):\n",
    "    return list(map(list, zip(*mylist)))\n",
    "\n",
    "def transpose_to_tensor(input_list):\n",
    "    make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n",
    "    return list(map(make_tensor, zip(*input_list)))\n",
    "\n",
    "\n",
    "\n",
    "class MADDPG:\n",
    "    \n",
    "    \n",
    "    def __init__(self, sharedActor=False, sharedCritic=False, discount_factor=0.95, tau=1.0e-2): #tau=1e-3):#0.02):\n",
    "        \n",
    "        super(MADDPG, self).__init__()      \n",
    "                       \n",
    "        #GREAT NETWORK MIGHT REQUIRE BATCH NORMALIZATION and might TRAIN BETTER\n",
    "        #WHIL SMALLER NETWORKS DON'T USUALLY REQUIRE BATCH NORMALIZATION BUT DON'T TRAIN BETTER WHEN THE DATA HAS A GREAT SIZE\n",
    "        #MAY BE BY NOT HAVING THE NETWORK TOO WIDE (LESS THAN 64) AND ADDING FEW MORE LAYERS, WE COULD AVOID BATCH NORMALIZATION WHITHOUT SACRIFICING TRAINING PERFORMANCE.\n",
    "        \n",
    "        \n",
    "        self.maddpg_agent = [DDPGAgent(48, 400, 300, 2, 48, 256, 128), #DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                             DDPGAgent(48, 400, 300, 2, 48, 256, 128)]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.maddpg_agent = [DDPGAgent(48, 96, 72, 2, 48, 72, 36), #DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                             DDPGAgent(48, 96, 72, 2, 48, 72, 36)]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.discount_factor = discount_factor#1 #No discount for this project #discount_factor\n",
    "        self.EPSILON_DECAY = 0.99\n",
    "        self.tau = tau\n",
    "        self.iter = 0\n",
    "        self.clipgrad = .1\n",
    "        self.sharedActor = sharedActor\n",
    "        self.sharedCritic = sharedCritic\n",
    "        \n",
    "        if sharedActor:\n",
    "            #merge the actor networks\n",
    "            for ddpgAgent in self.maddpg_agent:\n",
    "                ddpgAgent.actor = self.maddpg_agent[0].actor\n",
    "                ddpgAgent.target_actor = self.maddpg_agent[0].target_actor\n",
    "                \n",
    "        if sharedCritic:\n",
    "            #merge the critic networks\n",
    "            for ddpgAgent in self.maddpg_agent:\n",
    "                ddpgAgent.critic = self.maddpg_agent[0].critic\n",
    "                ddpgAgent.target_critic = self.maddpg_agent[0].target_critic                \n",
    "    \n",
    "    def get_actors(self):\n",
    "        \"\"\"get actors of all the agents in the MADDPG object\"\"\"\n",
    "        actors = [ddpg_agent.actor for ddpg_agent in self.maddpg_agent]\n",
    "        return actors\n",
    "\n",
    "    def get_target_actors(self):\n",
    "        \"\"\"get target_actors of all the agents in the MADDPG object\"\"\"\n",
    "        target_actors = [ddpg_agent.target_actor for ddpg_agent in self.maddpg_agent]\n",
    "        return target_actors\n",
    "\n",
    "    def act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"   \n",
    "        \n",
    "        obs_all_concatenated = []\n",
    "        obs_all_concatenated.append(np.concatenate((obs_all_agents[0],obs_all_agents[1])))\n",
    "        obs_all_concatenated.append(np.concatenate((obs_all_agents[0],obs_all_agents[1])))\n",
    "        actions = [agent.act(obs) for agent, obs in zip(self.maddpg_agent,obs_all_concatenated)]\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def target_act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "\n",
    "        target_actions = []\n",
    "        \n",
    "        for ddpg_agent, obs in zip(self.maddpg_agent, obs_all_agents):\n",
    "            tga_i = []\n",
    "            for ob in obs:\n",
    "                tga_i.append(ddpg_agent.target_act(ob, noise))\n",
    "            \n",
    "            target_actions.append(torch.stack(tga_i, dim=0))                \n",
    "\n",
    "        return target_actions\n",
    "\n",
    "        \n",
    "    def update(self, samples, agent_number):\n",
    "        \"\"\"update the critics and actors of all the agents \"\"\"\n",
    "\n",
    "        states, actions, rewards, next_states, dones = map(transpose_to_tensor, samples)\n",
    "\n",
    "        states[0] = states[0].to(device)\n",
    "        states[1] = states[1].to(device)\n",
    "\n",
    "        actions[0] = actions[0].to(device)\n",
    "        actions[1] = actions[1].to(device)\n",
    "        rewards = torch.stack(rewards)\n",
    "        rewards = rewards.to(device)\n",
    "\n",
    "        next_states[0] = next_states[0].to(device)\n",
    "        next_states[1] = next_states[1].to(device)\n",
    "                \n",
    "        dones = torch.stack(dones)\n",
    "        dones = dones.to(device)\n",
    "\n",
    "        states_full = torch.cat(states, dim=1) \n",
    "        states_full = states_full.to(device)\n",
    "\n",
    "        next_states_full = torch.cat(next_states, dim=1) \n",
    "        next_states_full = next_states_full.to(device)\n",
    "        agent = self.maddpg_agent[agent_number]\n",
    "        \n",
    "        #update critic network using policy gradient\n",
    "        \n",
    "        agent.critic_optimizer.zero_grad()\n",
    "        target_actions = agent.target_actor(next_states_full)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_next = agent.target_critic((next_states_full,target_actions.to(device)))\n",
    "        \n",
    "        y = rewards[agent_number].view(-1, 1) + self.discount_factor * q_next * (1 - dones[agent_number].view(-1, 1))\n",
    "        \n",
    "        q = agent.critic((states_full, actions[agent_number]))\n",
    "\n",
    "        huber_loss = torch.nn.SmoothL1Loss()\n",
    "        critic_loss = huber_loss(q, y.detach())\n",
    "        critic_loss.backward()\n",
    "        agent.critic_optimizer.step()\n",
    "\n",
    "        #update actor network using policy gradient\n",
    "        \n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        q_input = agent.actor(states_full)\n",
    "        \n",
    "        # get the policy gradient\n",
    "        actor_loss = -agent.critic((states_full, q_input.to(device))).mean()\n",
    "        actor_loss.backward()\n",
    "        agent.actor_optimizer.step()\n",
    "        \n",
    "\n",
    "    def update_targets(self):\n",
    "        \"\"\"soft update targets\"\"\"\n",
    "        self.iter += 1        \n",
    "        for ddpg_agent in self.maddpg_agent:\n",
    "            soft_update(ddpg_agent.target_actor, ddpg_agent.actor, self.tau)\n",
    "            soft_update(ddpg_agent.target_critic, ddpg_agent.critic, self.tau)\n",
    "            ddpg_agent.epsilon *= ddpg_agent.epsilon_decay\n",
    "            ddpg_agent.noise.reset()\n",
    "\n",
    "        \n",
    "class ReplayBuffer:\n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.deque = deque(maxlen=self.size)\n",
    "\n",
    "    def push(self,transition):\n",
    "        \"\"\"push into the buffer\"\"\"\n",
    "        \n",
    "        #Since we don't have many parallel environments, we can save the transition directly\n",
    "        self.deque.append(transition)\n",
    "\n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"sample from the buffer\"\"\"\n",
    "        samples = random.sample(self.deque, k=batchsize)\n",
    "\n",
    "        # transpose list of list\n",
    "        return transpose_list(samples)\n",
    "    \n",
    "    def extend(self, listToAdd):\n",
    "        \n",
    "        self.deque.extend(listToAdd)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def maddpgRunner(n_episodes=2000, max_t=100, print_every=100):\n",
    "    \n",
    "    start_time = int(time.time()) #training start time in seconds\n",
    "    \n",
    "    episodesCompletionTimes = []\n",
    "    \n",
    "    number_of_episodes = 1600 #1500 #1300 #n_episodes\n",
    "    episode_length = max_t\n",
    "    \n",
    "    batchsize = 128\n",
    "    TRAINING_ROUNDS = 8\n",
    "    \n",
    "    # how many episodes before update\n",
    "    episodes_per_update = 20\n",
    "    \n",
    "    BUFFER_SIZE = int(1000000)\n",
    "    buffer = ReplayBuffer(BUFFER_SIZE)        \n",
    "    \n",
    "    time_steps = 1    \n",
    "    \n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG(False, False, discount_factor= 0.99) #1) #there should be no discount factor for this project\n",
    "    \n",
    "    scores100 = deque(maxlen=print_every)       # A queue to keep only the last 100 episodes' scores\n",
    "    scores_deque = []                  # A list to keep all the max scores for plotting\n",
    "    \n",
    "    averagePrevScore100 = []   # A list that stores the average of the previous 100 max scores\n",
    "    \n",
    "    \n",
    "    solved = False\n",
    "    \n",
    "    currentMaxMean100 = -1\n",
    "    \n",
    "    for episode in range(1, number_of_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        score = np.zeros(num_agents)\n",
    "\n",
    "        while True:\n",
    "                    \n",
    "            actions = maddpg.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment            \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            score += env_info.rewards                         # update the score (for each agent)\n",
    "            \n",
    "            transition = (states, actions, rewards, next_states, dones)\n",
    "            buffer.push(list(transition))                        \n",
    "            \n",
    "            if len(buffer) >= batchsize and (time_steps % episodes_per_update) == 0:\n",
    "                \n",
    "                #train the 2 agents                                            \n",
    "                for _ in range(TRAINING_ROUNDS):\n",
    "                    for a_i in range(2): \n",
    "                        samples = buffer.sample(batchsize)\n",
    "                        maddpg.update(samples, a_i)\n",
    "                    maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                \n",
    "            states = next_states                               # roll over states to next time step\n",
    "            \n",
    "            time_steps +=1\n",
    "            \n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                    break   \n",
    "            \n",
    "            \n",
    "        max_score = max(score)                                    # Get the max score of the episode (over all agents)\n",
    "        scores_deque.append(max_score)                            # Store the score in the queue\n",
    "        scores100.append(max_score)                                  # Store the score in the list (for plotting)                \n",
    "            \n",
    "        meanScores100 = np.mean(scores100)\n",
    "        \n",
    "        averagePrevScore100.append(meanScores100)\n",
    "        \n",
    "        if meanScores100 > currentMaxMean100:\n",
    "            currentMaxMean100 = meanScores100\n",
    "        \n",
    "        episodesCompletionTimes.append(int((int(time.time()) - start_time)/60))\n",
    "        \n",
    "        \n",
    "        print('\\rEpisode {}  Average Score100: {:.4f}'.format(episode, meanScores100), end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print('\\rEpisode {} Average Score100: {:.4f}'.format(episode, meanScores100))\n",
    "        \n",
    "        mean_sc = np.mean(scores100)\n",
    "        if len(scores100) == 100 and mean_sc >= 0.5 and not solved:\n",
    "            print('\\rEnvironment solved in {} episodes, mean score: {:.4f}'.format(episode, mean_sc))\n",
    "            \n",
    "            solved = True\n",
    "            #print(episodesCompletionTimes)\n",
    "            #break\n",
    "    \n",
    "    agi = 0\n",
    "    \n",
    "    print('\\rMax average Score100 recorded: {:.4f}'.format(currentMaxMean100))\n",
    "    \n",
    "    for ddpgAgent in maddpg.maddpg_agent:\n",
    "        torch.save(ddpgAgent.actor.state_dict(), 'checkpoint_actor{}.pth'.format(agi))\n",
    "        torch.save(ddpgAgent.critic.state_dict(), 'checkpoint_critic{}.pth'.format(agi))\n",
    "        agi = agi + 1\n",
    "    \n",
    "    return (scores_deque, averagePrevScore100, episodesCompletionTimes)\n",
    "\n",
    "\n",
    "print(\"Starting Training\")\n",
    "scores = maddpgRunner()\n",
    "\n",
    "print(\"Training Ends\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')#constrained_layout=True)\n",
    "ax.plot(np.arange(1, len(scores[0])+1), scores[0], label='Max Rewards')\n",
    "ax.plot(np.arange(1, len(scores[1])+1), scores[1], label='Average100 Max Rewards')\n",
    "ax.set_xlabel('Episode #')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Tennis Training Chart')\n",
    "\n",
    "secax = ax.twiny()\n",
    "\n",
    "secax.plot(scores[2], scores[1], color='lightgreen')\n",
    "secax.set_xlabel('Time [min]')\n",
    "\n",
    "secax.spines['top'].set_color('green')\n",
    "secax.xaxis.label.set_color('green')\n",
    "secax.tick_params(axis='x', colors='green')\n",
    "\n",
    "plt.grid(True, color='lightgray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
