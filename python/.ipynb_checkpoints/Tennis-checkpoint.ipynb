{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /workspace/home/python\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (1.26.3)\n",
      "Requirement already satisfied: jupyter in /home/student/.local/lib/python3.11/site-packages (from unityagents==0.4.0) (1.1.1)\n",
      "Requirement already satisfied: pytest>=3.2.2 in /home/student/.local/lib/python3.11/site-packages (from unityagents==0.4.0) (8.4.1)\n",
      "Requirement already satisfied: docopt in /home/student/.local/lib/python3.11/site-packages (from unityagents==0.4.0) (0.6.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (6.0.1)\n",
      "Requirement already satisfied: protobuf>=3.5.2 in /home/student/.local/lib/python3.11/site-packages (from unityagents==0.4.0) (3.20.1)\n",
      "Requirement already satisfied: grpcio>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (1.58.0)\n",
      "Requirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (2.1.4)\n",
      "Requirement already satisfied: scipy==1.12.0 in /home/student/.local/lib/python3.11/site-packages (from unityagents==0.4.0) (1.12.0)\n",
      "Requirement already satisfied: ipykernel==6.29.4 in /usr/local/lib/python3.11/dist-packages (from unityagents==0.4.0) (6.29.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (8.24.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (1.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.29.4->unityagents==0.4.0) (5.14.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.4->unityagents==0.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.4->unityagents==0.4.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.4->unityagents==0.4.0) (2024.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /home/student/.local/lib/python3.11/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/student/.local/lib/python3.11/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest>=3.2.2->unityagents==0.4.0) (2.18.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->unityagents==0.4.0) (6.5.6)\n",
      "Requirement already satisfied: jupyter-console in /home/student/.local/lib/python3.11/site-packages (from jupyter->unityagents==0.4.0) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->unityagents==0.4.0) (7.16.4)\n",
      "Requirement already satisfied: ipywidgets in /home/student/.local/lib/python3.11/site-packages (from jupyter->unityagents==0.4.0) (8.1.7)\n",
      "Requirement already satisfied: jupyterlab in /home/student/.local/lib/python3.11/site-packages (from jupyter->unityagents==0.4.0) (4.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unityagents==0.4.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unityagents==0.4.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unityagents==0.4.0) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unityagents==0.4.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->unityagents==0.4.0) (2.4.7)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (3.0.43)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/student/.local/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (4.14.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (4.9.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel==6.29.4->unityagents==0.4.0) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.4->unityagents==0.4.0) (4.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.4->unityagents==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/student/.local/lib/python3.11/site-packages (from ipywidgets->jupyter->unityagents==0.4.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/student/.local/lib/python3.11/site-packages (from ipywidgets->jupyter->unityagents==0.4.0) (3.0.15)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/student/.local/lib/python3.11/site-packages (from jupyterlab->jupyter->unityagents==0.4.0) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/student/.local/lib/python3.11/site-packages (from jupyterlab->jupyter->unityagents==0.4.0) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->unityagents==0.4.0) (3.1.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/student/.local/lib/python3.11/site-packages (from jupyterlab->jupyter->unityagents==0.4.0) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->unityagents==0.4.0) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /home/student/.local/lib/python3.11/site-packages (from jupyterlab->jupyter->unityagents==0.4.0) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->unityagents==0.4.0) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->unityagents==0.4.0) (68.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->unityagents==0.4.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->unityagents==0.4.0) (23.1.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->unityagents==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->unityagents==0.4.0) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->unityagents==0.4.0) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->unityagents==0.4.0) (0.20.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->unityagents==0.4.0) (1.0.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->nbconvert->jupyter->unityagents==0.4.0) (0.5.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->unityagents==0.4.0) (4.3.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->unityagents==0.4.0) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/student/.local/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter->unityagents==0.4.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->unityagents==0.4.0) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /home/student/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->unityagents==0.4.0) (0.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (0.8.4)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (7.7.0)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->unityagents==0.4.0) (21.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /home/student/.local/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/student/.local/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (4.22.0)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->unityagents==0.4.0) (2.19.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->unityagents==0.4.0) (2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=7.23.1->ipykernel==6.29.4->unityagents==0.4.0) (0.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->unityagents==0.4.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (0.18.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->unityagents==0.4.0) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->unityagents==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->unityagents==0.4.0) (2.22)\n",
      "Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (2.4)\n",
      "Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->unityagents==0.4.0) (2.9.0.20240316)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: unityagents\n",
      "  Building wheel for unityagents (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unityagents: filename=unityagents-0.4.0-py3-none-any.whl size=71615 sha256=8dea7f749dda2cafd0a8d880618e716042e77d58455ff6256b5b525f838fb7a0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k0__n9te/wheels/7a/40/00/46cdf9603f1faf1c020d0c841b78009acc81cf9b5d130b0fc3\n",
      "Successfully built unityagents\n",
      "Installing collected packages: unityagents\n",
      "  Attempting uninstall: unityagents\n",
      "    Found existing installation: unityagents 0.4.0\n",
      "    Uninstalling unityagents-0.4.0:\n",
      "      Successfully uninstalled unityagents-0.4.0\n",
      "Successfully installed unityagents-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/student/.local/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /home/student/.local/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in /home/student/.local/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/student/.local/lib/python3.11/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/student/.local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/student/.local/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/student/.local/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/student/.local/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/student/.local/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/student/.local/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/student/.local/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/student/.local/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/student/.local/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/student/.local/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (68.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: protobuf==3.20.1 in /home/student/.local/lib/python3.11/site-packages (3.20.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.1 #protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "Restart the Kernel, and verify the protobuf version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.20.1\n"
     ]
    }
   ],
   "source": [
    "import google.protobuf\n",
    "print(google.protobuf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Instructions\n",
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /data/Tennis_Linux_NoVis/Tennis.x86_64\n",
      "Mono path[0] = '/data/Tennis_Linux_NoVis/Tennis_Data/Managed'\n",
      "Mono config path = '/data/Tennis_Linux_NoVis/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/student/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.4669857  -1.5\n",
      "  0.          0.         -6.83172083  6.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -7.38993645 -1.5        -0.          0.\n",
      "  6.83172083  5.99607611 -0.          0.         -6.28996849 -0.98316395\n",
      " 10.99967289  6.21520042  6.83172083  5.91759634 10.99967289  6.21520042]\n",
      "The state for the second agent looks like: [  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.          -6.70024681  -1.5\n",
      "   0.           0.          -6.83172083   5.99607611   0.\n",
      "   0.          -8.67523003  -1.55886006 -19.74982262  -0.98100001\n",
      "  -6.83172083   5.91759634 -19.74982262  -0.98100001]\n",
      "The state for the first agent looks like: [-7.38993645 -1.5        -0.          0.          6.83172083  5.99607611\n",
      " -0.          0.         -6.28996849 -0.98316395 10.99967289  6.21520042\n",
      "  6.83172083  5.91759634 10.99967289  6.21520042 -6.60046339 -0.42050385\n",
      " -3.10494757  5.23420095  6.83172083  5.74101639 -3.10494757  5.23420095]\n",
      "The state for the second agent looks like: [ -6.70024681  -1.5          0.           0.          -6.83172083\n",
      "   5.99607611   0.           0.          -8.67523003  -1.55886006\n",
      " -19.74982262  -0.98100001  -6.83172083   5.91759634 -19.74982262\n",
      "  -0.98100001  -9.02195263  -0.91772008  -3.46723914   6.01900053\n",
      "  -6.83172083   5.74101639  -3.46723914   6.01900053]\n",
      "The state for the first agent looks like: [-6.28996849 -0.98316395 10.99967289  6.21520042  6.83172083  5.91759634\n",
      " 10.99967289  6.21520042 -6.60046339 -0.42050385 -3.10494757  5.23420095\n",
      "  6.83172083  5.74101639 -3.10494757  5.23420095 -5.45156479  0.0440563\n",
      " 11.4889946   4.25320148  6.83172083  5.46633625 11.4889946   4.25320148]\n",
      "The state for the second agent looks like: [ -8.67523003  -1.55886006 -19.74982262  -0.98100001  -6.83172083\n",
      "   5.91759634 -19.74982262  -0.98100001  -9.02195263  -0.91772008\n",
      "  -3.46723914   6.01900053  -6.83172083   5.74101639  -3.46723914\n",
      "   6.01900053 -10.95613289  -0.37467998   0.           5.03800106\n",
      "  -6.83172083   5.46633625   0.           5.03800106]\n",
      "The state for the first agent looks like: [-6.60046339 -0.42050385 -3.10494757  5.23420095  6.83172083  5.74101639\n",
      " -3.10494757  5.23420095 -5.45156479  0.0440563  11.4889946   4.25320148\n",
      "  6.83172083  5.46633625 11.4889946   4.25320148 -6.30698919  0.41051644\n",
      " -8.55424786  3.27220201  6.83172083  5.0935564  -8.55424786  3.27220201]\n",
      "The state for the second agent looks like: [ -9.02195263  -0.91772008  -3.46723914   6.01900053  -6.83172083\n",
      "   5.74101639  -3.46723914   6.01900053 -10.95613289  -0.37467998\n",
      "   0.           5.03800106  -6.83172083   5.46633625   0.\n",
      "   5.03800106 -10.89980793   0.07026011   0.           4.05700159\n",
      "  -6.83172083   5.0935564    0.           4.05700159]\n",
      "The state for the first agent looks like: [-5.45156479  0.0440563  11.4889946   4.25320148  6.83172083  5.46633625\n",
      " 11.4889946   4.25320148 -6.30698919  0.41051644 -8.55424786  3.27220201\n",
      "  6.83172083  5.0935564  -8.55424786  3.27220201 -5.3800559   0.67887664\n",
      "  9.26933384  2.29120255  6.83172083  4.62267637  9.26933384  2.29120255]\n",
      "The state for the second agent looks like: [-1.09561329e+01 -3.74679983e-01  0.00000000e+00  5.03800106e+00\n",
      " -6.83172083e+00  5.46633625e+00  0.00000000e+00  5.03800106e+00\n",
      " -1.08998079e+01  7.02601075e-02  0.00000000e+00  4.05700159e+00\n",
      " -6.83172083e+00  5.09355640e+00  0.00000000e+00  4.05700159e+00\n",
      " -1.08997898e+01  4.17100310e-01 -9.53674316e-06  3.07600212e+00\n",
      " -6.83172083e+00  4.62267637e+00 -9.53674316e-06  3.07600212e+00]\n",
      "The state for the first agent looks like: [ -6.30698919   0.41051644  -8.55424786   3.27220201   6.83172083\n",
      "   5.0935564   -8.55424786   3.27220201  -5.3800559    0.67887664\n",
      "   9.26933384   2.29120255   6.83172083   4.62267637   9.26933384\n",
      "   2.29120255  -6.98972464   0.84913683 -16.09669495   1.3102026\n",
      "   6.83172083   4.05369663 -16.09669495   1.3102026 ]\n",
      "The state for the second agent looks like: [-1.08998079e+01  7.02601075e-02  0.00000000e+00  4.05700159e+00\n",
      " -6.83172083e+00  5.09355640e+00  0.00000000e+00  4.05700159e+00\n",
      " -1.08997898e+01  4.17100310e-01 -9.53674316e-06  3.07600212e+00\n",
      " -6.83172083e+00  4.62267637e+00 -9.53674316e-06  3.07600212e+00\n",
      " -1.08997898e+01  6.65840507e-01 -9.53674316e-06  2.09500265e+00\n",
      " -6.83172083e+00  4.05369663e+00 -9.53674316e-06  2.09500265e+00]\n",
      "The state for the first agent looks like: [ -5.3800559    0.67887664   9.26933384   2.29120255   6.83172083\n",
      "   4.62267637   9.26933384   2.29120255  -6.98972464   0.84913683\n",
      " -16.09669495   1.3102026    6.83172083   4.05369663 -16.09669495\n",
      "   1.3102026   -5.58773041   0.92129707  14.01994324   0.32920253\n",
      "   6.83172083   3.38661671  14.01994324   0.32920253]\n",
      "The state for the second agent looks like: [-1.08997898e+01  4.17100310e-01 -9.53674316e-06  3.07600212e+00\n",
      " -6.83172083e+00  4.62267637e+00 -9.53674316e-06  3.07600212e+00\n",
      " -1.08997898e+01  6.65840507e-01 -9.53674316e-06  2.09500265e+00\n",
      " -6.83172083e+00  4.05369663e+00 -9.53674316e-06  2.09500265e+00\n",
      " -9.10334873e+00  8.16480696e-01  1.79644241e+01  1.11400259e+00\n",
      " -6.83172083e+00  3.38661671e+00  1.79644241e+01  1.11400259e+00]\n",
      "The state for the first agent looks like: [ -6.98972464   0.84913683 -16.09669495   1.3102026    6.83172083\n",
      "   4.05369663 -16.09669495   1.3102026   -5.58773041   0.92129707\n",
      "  14.01994324   0.32920253   6.83172083   3.38661671  14.01994324\n",
      "   0.32920253  -5.42835474   0.89535731   1.59375393  -0.65179747\n",
      "   6.83172083   2.62143707   1.59375393  -0.65179747]\n",
      "The state for the second agent looks like: [-1.08997898e+01  6.65840507e-01 -9.53674316e-06  2.09500265e+00\n",
      " -6.83172083e+00  4.05369663e+00 -9.53674316e-06  2.09500265e+00\n",
      " -9.10334873e+00  8.16480696e-01  1.79644241e+01  1.11400259e+00\n",
      " -6.83172083e+00  3.38661671e+00  1.79644241e+01  1.11400259e+00\n",
      " -6.10334778e+00  8.69020939e-01  3.00000000e+01  1.33002535e-01\n",
      " -6.83172083e+00  2.62143707e+00  3.00000000e+01  1.33002535e-01]\n",
      "The state for the first agent looks like: [-5.58773041  0.92129707 14.01994324  0.32920253  6.83172083  3.38661671\n",
      " 14.01994324  0.32920253 -5.42835474  0.89535731  1.59375393 -0.65179747\n",
      "  6.83172083  2.62143707  1.59375393 -0.65179747 -6.20428324  0.7713176\n",
      " -7.75927401 -1.63279748  6.83172083  1.75815713 -7.75927401 -1.63279748]\n",
      "The state for the second agent looks like: [-9.10334873  0.8164807  17.96442413  1.11400259 -6.83172083  3.38661671\n",
      " 17.96442413  1.11400259 -6.10334778  0.86902094 30.          0.13300253\n",
      " -6.83172083  2.62143707 30.          0.13300253 -4.64287901  0.82346117\n",
      " 14.60469437 -0.84799749 -6.83172083  1.75815713 14.60469437 -0.84799749]\n",
      "The state for the first agent looks like: [-5.42835474  0.89535731  1.59375393 -0.65179747  6.83172083  2.62143707\n",
      "  1.59375393 -0.65179747 -6.20428324  0.7713176  -7.75927401 -1.63279748\n",
      "  6.83172083  1.75815713 -7.75927401 -1.63279748 -3.31945658  0.54917783\n",
      " 28.84827995 -2.61379719  6.83172083  0.83853728 28.84827995 -2.61379719]\n",
      "The state for the second agent looks like: [-6.10334778  0.86902094 30.          0.13300253 -6.83172083  2.62143707\n",
      " 30.          0.13300253 -4.64287901  0.82346117 14.60469437 -0.84799749\n",
      " -6.83172083  1.75815713 14.60469437 -0.84799749 -1.64287972  0.67980146\n",
      " 30.         -1.82899749 -6.83172083  0.83853728 30.         -1.82899749]\n",
      "The state for the first agent looks like: [-6.20428324  0.7713176  -7.75927401 -1.63279748  6.83172083  1.75815713\n",
      " -7.75927401 -1.63279748 -3.31945658  0.54917783 28.84827995 -2.61379719\n",
      "  6.83172083  0.83853728 28.84827995 -2.61379719 -1.47740722  0.22893813\n",
      " 18.42049026 -3.59479666  6.83172083 -0.08108279 18.42049026 -3.59479666]\n",
      "The state for the second agent looks like: [ -4.64287901   0.82346117  14.60469437  -0.84799749  -6.83172083\n",
      "   1.75815713  14.60469437  -0.84799749  -1.64287972   0.67980146\n",
      "  30.          -1.82899749  -6.83172083   0.83853728  30.\n",
      "  -1.82899749  -4.51706076   0.43804172 -28.74180794  -2.80999708\n",
      "  -6.83172083  -0.08108279 -28.74180794  -2.80999708]\n",
      "The state for the first agent looks like: [-3.31945658  0.54917783 28.84827995 -2.61379719  6.83172083  0.83853728\n",
      " 28.84827995 -2.61379719 -1.47740722  0.22893813 18.42049026 -3.59479666\n",
      "  6.83172083 -0.08108279 18.42049026 -3.59479666 -0.39999998 -0.18940151\n",
      " 30.         -4.57579613  7.81257772 -1.4109621  30.         -4.57579613]\n",
      "The state for the second agent looks like: [ -1.64287972   0.67980146  30.          -1.82899749  -6.83172083\n",
      "   0.83853728  30.          -1.82899749  -4.51706076   0.43804172\n",
      " -28.74180794  -2.80999708  -6.83172083  -0.08108279 -28.74180794\n",
      "  -2.80999708  -7.4343996    0.40587658 -30.           0.05518503\n",
      "  -7.81257772  -1.4109621  -30.           0.05518503]\n",
      "The state for the first agent looks like: [ -1.47740722   0.22893813  18.42049026  -3.59479666   6.83172083\n",
      "  -0.08108279  18.42049026  -3.59479666  -0.39999998  -0.18940151\n",
      "  30.          -4.57579613   7.81257772  -1.4109621   30.\n",
      "  -4.57579613  -3.99999976  -0.70584118 -30.          -5.5567956\n",
      "   8.71257877  -2.3305819  -30.          -5.5567956 ]\n",
      "The state for the second agent looks like: [ -4.51706076   0.43804172 -28.74180794  -2.80999708  -6.83172083\n",
      "  -0.08108279 -28.74180794  -2.80999708  -7.4343996    0.40587658\n",
      " -30.           0.05518503  -7.81257772  -1.4109621  -30.\n",
      "   0.05518503  -5.56094694   0.35253507  18.73452568  -0.92581499\n",
      "  -8.71257877  -2.3305819   18.73452568  -0.92581499]\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -7.43639946 -1.5\n",
      " -0.          0.          6.69487906  5.99607611 -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -7.73019552 -1.5\n",
      "  0.          0.         -6.69487906  5.99607611  0.          0.        ]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.24261761 -1.5        -0.          0.\n",
      "  6.69487906  5.98822832 -0.          0.         -6.35256195 -1.55886006\n",
      " -1.09944546 -0.98100001  6.69487906  5.89012814 -1.09944546 -0.98100001]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.72329426 -1.5         0.          0.\n",
      " -6.69487906  5.98822832  0.          0.         -5.13993263 -1.55886006\n",
      " 15.83362389 -0.98100001 -6.69487906  5.89012814 15.83362389 -0.98100001]\n",
      "The state for the first agent looks like: [ -6.24261761  -1.5         -0.           0.           6.69487906\n",
      "   5.98822832  -0.           0.          -6.35256195  -1.55886006\n",
      "  -1.09944546  -0.98100001   6.69487906   5.89012814  -1.09944546\n",
      "  -0.98100001  -8.24891376  -1.71581995 -18.96351433  -1.96200001\n",
      "   6.69487906   5.69392824 -18.96351433  -1.96200001]\n",
      "The state for the second agent looks like: [-6.72329426 -1.5         0.          0.         -6.69487906  5.98822832\n",
      "  0.          0.         -5.13993263 -1.55886006 15.83362389 -0.98100001\n",
      " -6.69487906  5.89012814 15.83362389 -0.98100001 -3.04229283 -1.71581995\n",
      " 20.97640228 -1.96200001 -6.69487906  5.69392824 20.97640228 -1.96200001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state for the first agent looks like: [ -6.35256195  -1.55886006  -1.09944546  -0.98100001   6.69487906\n",
      "   5.89012814  -1.09944546  -0.98100001  -8.24891376  -1.71581995\n",
      " -18.96351433  -1.96200001   6.69487906   5.69392824 -18.96351433\n",
      "  -1.96200001  -8.61370373  -1.05898392  -3.64791346   6.21520042\n",
      "   6.69487906   5.39962816  -3.64791346   6.21520042]\n",
      "The state for the second agent looks like: [-5.13993263 -1.55886006 15.83362389 -0.98100001 -6.69487906  5.89012814\n",
      " 15.83362389 -0.98100001 -3.04229283 -1.71581995 20.97640228 -1.96200001\n",
      " -6.69487906  5.69392824 20.97640228 -1.96200001 -3.11586642 -1.85235918\n",
      " -0.73573524  0.         -6.69487906  5.39962816 -0.73573524  0.        ]\n",
      "The state for the first agent looks like: [ -8.24891376  -1.71581995 -18.96351433  -1.96200001   6.69487906\n",
      "   5.69392824 -18.96351433  -1.96200001  -8.61370373  -1.05898392\n",
      "  -3.64791346   6.21520042   6.69487906   5.39962816  -3.64791346\n",
      "   6.21520042 -10.92257309  -0.49632382  -0.           5.23420095\n",
      "   6.69487906   5.00722837  -0.           5.23420095]\n",
      "The state for the second agent looks like: [-3.04229283 -1.71581995 20.97640228 -1.96200001 -6.69487906  5.69392824\n",
      " 20.97640228 -1.96200001 -3.11586642 -1.85235918 -0.73573524  0.\n",
      " -6.69487906  5.39962816 -0.73573524  0.         -0.39999998 -1.852162\n",
      " 30.          0.         -6.69487906  5.00722837 30.          0.        ]\n",
      "The state for the first agent looks like: [ -8.61370373  -1.05898392  -3.64791346   6.21520042   6.69487906\n",
      "   5.39962816  -3.64791346   6.21520042 -10.92257309  -0.49632382\n",
      "  -0.           5.23420095   6.69487906   5.00722837  -0.\n",
      "   5.23420095  -7.92257071  -0.0317637   30.           4.25320148\n",
      "   6.69487906   4.5167284   30.           4.25320148]\n",
      "The state for the second agent looks like: [-3.11586642e+00 -1.85235918e+00 -7.35735238e-01  0.00000000e+00\n",
      " -6.69487906e+00  5.39962816e+00 -7.35735238e-01  0.00000000e+00\n",
      " -3.99999976e-01 -1.85216200e+00  3.00000000e+01  0.00000000e+00\n",
      " -6.69487906e+00  5.00722837e+00  3.00000000e+01  0.00000000e+00\n",
      " -3.99999976e+00 -1.85216200e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  4.51672840e+00 -3.00000000e+01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [-10.92257309  -0.49632382  -0.           5.23420095   6.69487906\n",
      "   5.00722837  -0.           5.23420095  -7.92257071  -0.0317637\n",
      "  30.           4.25320148   6.69487906   4.5167284   30.\n",
      "   4.25320148  -4.92257118   0.33469647  30.           3.27220201\n",
      "   6.69487906   3.92812872  30.           3.27220201]\n",
      "The state for the second agent looks like: [-3.99999976e-01 -1.85216200e+00  3.00000000e+01  0.00000000e+00\n",
      " -6.69487906e+00  5.00722837e+00  3.00000000e+01  0.00000000e+00\n",
      " -3.99999976e+00 -1.85216200e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  4.51672840e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -1.00000024e+00 -1.85216200e+00  3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  3.92812872e+00  3.00000000e+01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ -7.92257071  -0.0317637   30.           4.25320148   6.69487906\n",
      "   4.5167284   30.           4.25320148  -4.92257118   0.33469647\n",
      "  30.           3.27220201   6.69487906   3.92812872  30.\n",
      "   3.27220201  -6.31104231   0.60305667 -13.88471413   2.29120255\n",
      "   6.69487906   3.24142885 -13.88471413   2.29120255]\n",
      "The state for the second agent looks like: [-3.99999976e+00 -1.85216200e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  4.51672840e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -1.00000024e+00 -1.85216200e+00  3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  3.92812872e+00  3.00000000e+01 -4.76837158e-06\n",
      " -1.47992730e+00 -1.85216200e+00 -4.79926825e+00 -4.76837158e-06\n",
      " -6.69487906e+00  3.24142885e+00 -4.79926825e+00 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ -4.92257118   0.33469647  30.           3.27220201   6.69487906\n",
      "   3.92812872  30.           3.27220201  -6.31104231   0.60305667\n",
      " -13.88471413   2.29120255   6.69487906   3.24142885 -13.88471413\n",
      "   2.29120255  -9.31104374   0.77331686 -30.           1.3102026\n",
      "   6.69487906   2.45662904 -30.           1.3102026 ]\n",
      "The state for the second agent looks like: [-1.00000024e+00 -1.85216200e+00  3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  3.92812872e+00  3.00000000e+01 -4.76837158e-06\n",
      " -1.47992730e+00 -1.85216200e+00 -4.79926825e+00 -4.76837158e-06\n",
      " -6.69487906e+00  3.24142885e+00 -4.79926825e+00 -4.76837158e-06\n",
      " -1.56136394e+00 -1.85216200e+00 -8.14371049e-01 -4.76837158e-06\n",
      " -6.69487906e+00  2.45662904e+00 -8.14371049e-01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ -6.31104231   0.60305667 -13.88471413   2.29120255   6.69487906\n",
      "   3.24142885 -13.88471413   2.29120255  -9.31104374   0.77331686\n",
      " -30.           1.3102026    6.69487906   2.45662904 -30.\n",
      "   1.3102026   -6.31104279   0.8454771   30.           0.32920253\n",
      "   6.69487906   1.57423317  30.           0.32920253]\n",
      "The state for the second agent looks like: [-1.47992730e+00 -1.85216200e+00 -4.79926825e+00 -4.76837158e-06\n",
      " -6.69487906e+00  3.24142885e+00 -4.79926825e+00 -4.76837158e-06\n",
      " -1.56136394e+00 -1.85216200e+00 -8.14371049e-01 -4.76837158e-06\n",
      " -6.69487906e+00  2.45662904e+00 -8.14371049e-01 -4.76837158e-06\n",
      " -4.56136322e+00 -1.85216200e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  1.57423317e+00 -3.00000000e+01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ -9.31104374   0.77331686 -30.           1.3102026    6.69487906\n",
      "   2.45662904 -30.           1.3102026   -6.31104279   0.8454771\n",
      "  30.           0.32920253   6.69487906   1.57423317  30.\n",
      "   0.32920253  -3.73366666   0.81953728  25.77375984  -0.65179747\n",
      "   6.69487906   0.65461326  25.77375984  -0.65179747]\n",
      "The state for the second agent looks like: [-1.56136394e+00 -1.85216200e+00 -8.14371049e-01 -4.76837158e-06\n",
      " -6.69487906e+00  2.45662904e+00 -8.14371049e-01 -4.76837158e-06\n",
      " -4.56136322e+00 -1.85216200e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  1.57423317e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -5.85442162e+00 -1.85216200e+00 -1.29305773e+01 -4.76837158e-06\n",
      " -6.69487906e+00  6.54613256e-01 -1.29305773e+01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [-6.31104279  0.8454771  30.          0.32920253  6.69487906  1.57423317\n",
      " 30.          0.32920253 -3.73366666  0.81953728 25.77375984 -0.65179747\n",
      "  6.69487906  0.65461326 25.77375984 -0.65179747 -0.73366708  0.69549763\n",
      " 30.         -1.63279748  6.69487906 -0.26500678 30.         -1.63279748]\n",
      "The state for the second agent looks like: [-4.56136322e+00 -1.85216200e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -6.69487906e+00  1.57423317e+00 -3.00000000e+01 -4.76837158e-06\n",
      " -5.85442162e+00 -1.85216200e+00 -1.29305773e+01 -4.76837158e-06\n",
      " -6.69487906e+00  6.54613256e-01 -1.29305773e+01 -4.76837158e-06\n",
      " -8.19383907e+00 -1.85216200e+00 -2.33941784e+01 -4.76837158e-06\n",
      " -6.69487906e+00 -2.65006781e-01 -2.33941784e+01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ -3.73366666   0.81953728  25.77375984  -0.65179747   6.69487906\n",
      "   0.65461326  25.77375984  -0.65179747  -0.73366708   0.69549763\n",
      "  30.          -1.63279748   6.69487906  -0.26500678  30.\n",
      "  -1.63279748  -2.35464072   0.47335786 -13.54640484  -2.61379719\n",
      "   6.69487906  -1.18462682 -13.54640484  -2.61379719]\n",
      "The state for the second agent looks like: [-5.85442162e+00 -1.85216200e+00 -1.29305773e+01 -4.76837158e-06\n",
      " -6.69487906e+00  6.54613256e-01 -1.29305773e+01 -4.76837158e-06\n",
      " -8.19383907e+00 -1.85216200e+00 -2.33941784e+01 -4.76837158e-06\n",
      " -6.69487906e+00 -2.65006781e-01 -2.33941784e+01 -4.76837158e-06\n",
      " -1.10434227e+01 -1.85216200e+00 -2.84958553e+01 -4.76837158e-06\n",
      " -6.69487906e+00 -1.18462682e+00 -2.84958553e+01 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ -0.73366708   0.69549763  30.          -1.63279748   6.69487906\n",
      "  -0.26500678  30.          -1.63279748  -2.35464072   0.47335786\n",
      " -13.54640484  -2.61379719   6.69487906  -1.18462682 -13.54640484\n",
      "  -2.61379719  -1.38131237   0.15311816   9.73328304  -3.59479666\n",
      "   6.69487906  -2.10424662   9.73328304  -3.59479666]\n",
      "The state for the second agent looks like: [-8.19383907e+00 -1.85216200e+00 -2.33941784e+01 -4.76837158e-06\n",
      " -6.69487906e+00 -2.65006781e-01 -2.33941784e+01 -4.76837158e-06\n",
      " -1.10434227e+01 -1.85216200e+00 -2.84958553e+01 -4.76837158e-06\n",
      " -6.69487906e+00 -1.18462682e+00 -2.84958553e+01 -4.76837158e-06\n",
      " -1.08998356e+01 -1.85216200e+00  0.00000000e+00 -4.76837158e-06\n",
      " -6.69487906e+00 -2.10424662e+00  0.00000000e+00 -4.76837158e-06]\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.59263992 -1.5\n",
      " -0.          0.          6.55063343  6.         -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -7.37911606 -1.5\n",
      "  0.          0.         -6.55063343  6.          0.          0.        ]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -7.08839989 -1.5        -0.          0.\n",
      "  6.55063343  5.99607611 -0.          0.         -5.00625706 -0.98316395\n",
      " 20.82143784  6.21520042  6.55063343  5.91759634 20.82143784  6.21520042]\n",
      "The state for the second agent looks like: [  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.          -7.44412899  -1.5\n",
      "   0.           0.          -6.55063343   5.99607611   0.\n",
      "   0.          -9.64926338  -1.55886006 -22.05135727  -0.98100001\n",
      "  -6.55063343   5.91759634 -22.05135727  -0.98100001]\n",
      "The state for the first agent looks like: [-7.08839989 -1.5        -0.          0.          6.55063343  5.99607611\n",
      " -0.          0.         -5.00625706 -0.98316395 20.82143784  6.21520042\n",
      "  6.55063343  5.91759634 20.82143784  6.21520042 -2.00625777 -0.42050385\n",
      " 30.          5.23420095  6.55063343  5.74101639 30.          5.23420095]\n",
      "The state for the second agent looks like: [ -7.44412899  -1.5          0.           0.          -6.55063343\n",
      "   5.99607611   0.           0.          -9.64926338  -1.55886006\n",
      " -22.05135727  -0.98100001  -6.55063343   5.91759634 -22.05135727\n",
      "  -0.98100001 -10.64699745  -0.91772008  -9.97732353   6.01900053\n",
      "  -6.55063343   5.74101639  -9.97732353   6.01900053]\n",
      "The state for the first agent looks like: [-5.00625706 -0.98316395 20.82143784  6.21520042  6.55063343  5.91759634\n",
      " 20.82143784  6.21520042 -2.00625777 -0.42050385 30.          5.23420095\n",
      "  6.55063343  5.74101639 30.          5.23420095 -0.6854257   0.0440563\n",
      " 15.72871304  4.25320148  6.55063343  5.46633625 15.72871304  4.25320148]\n",
      "The state for the second agent looks like: [ -9.64926338  -1.55886006 -22.05135727  -0.98100001  -6.55063343\n",
      "   5.91759634 -22.05135727  -0.98100001 -10.64699745  -0.91772008\n",
      "  -9.97732353   6.01900053  -6.55063343   5.74101639  -9.97732353\n",
      "   6.01900053  -7.64699554  -0.37467998  30.           5.03800106\n",
      "  -6.55063343   5.46633625  30.           5.03800106]\n",
      "The state for the first agent looks like: [-2.00625777 -0.42050385 30.          5.23420095  6.55063343  5.74101639\n",
      " 30.          5.23420095 -0.6854257   0.0440563  15.72871304  4.25320148\n",
      "  6.55063343  5.46633625 15.72871304  4.25320148 -0.41571206  0.41051644\n",
      " 29.21439362  3.27220201  6.55063343  5.0935564  29.21439362  3.27220201]\n",
      "The state for the second agent looks like: [-10.64699745  -0.91772008  -9.97732353   6.01900053  -6.55063343\n",
      "   5.74101639  -9.97732353   6.01900053  -7.64699554  -0.37467998\n",
      "  30.           5.03800106  -6.55063343   5.46633625  30.\n",
      "   5.03800106  -8.92084312   0.07026011 -12.73847485   4.05700159\n",
      "  -6.55063343   5.0935564  -12.73847485   4.05700159]\n",
      "The state for the first agent looks like: [-0.6854257   0.0440563  15.72871304  4.25320148  6.55063343  5.46633625\n",
      " 15.72871304  4.25320148 -0.41571206  0.41051644 29.21439362  3.27220201\n",
      "  6.55063343  5.0935564  29.21439362  3.27220201 -1.25472999  0.67887664\n",
      " -2.54730654  2.29120255  6.55063343  4.62267637 -2.54730654  2.29120255]\n",
      "The state for the second agent looks like: [ -7.64699554  -0.37467998  30.           5.03800106  -6.55063343\n",
      "   5.46633625  30.           5.03800106  -8.92084312   0.07026011\n",
      " -12.73847485   4.05700159  -6.55063343   5.0935564  -12.73847485\n",
      "   4.05700159  -9.272892     0.41710031  -3.52050424   3.07600212\n",
      "  -6.55063343   4.62267637  -3.52050424   3.07600212]\n",
      "The state for the first agent looks like: [ -0.41571206   0.41051644  29.21439362   3.27220201   6.55063343\n",
      "   5.0935564   29.21439362   3.27220201  -1.25472999   0.67887664\n",
      "  -2.54730654   2.29120255   6.55063343   4.62267637  -2.54730654\n",
      "   2.29120255  -4.25472927   0.84913683 -30.           1.3102026\n",
      "   6.55063343   4.05369663 -30.           1.3102026 ]\n",
      "The state for the second agent looks like: [ -8.92084312   0.07026011 -12.73847485   4.05700159  -6.55063343\n",
      "   5.0935564  -12.73847485   4.05700159  -9.272892     0.41710031\n",
      "  -3.52050424   3.07600212  -6.55063343   4.62267637  -3.52050424\n",
      "   3.07600212  -8.93501472   0.66584051   3.3787868    2.09500265\n",
      "  -6.55063343   4.05369663   3.3787868    2.09500265]\n",
      "The state for the first agent looks like: [ -1.25472999   0.67887664  -2.54730654   2.29120255   6.55063343\n",
      "   4.62267637  -2.54730654   2.29120255  -4.25472927   0.84913683\n",
      " -30.           1.3102026    6.55063343   4.05369663 -30.\n",
      "   1.3102026   -1.25472999   0.92129707  30.           0.32920253\n",
      "   6.55063343   3.38661671  30.           0.32920253]\n",
      "The state for the second agent looks like: [-9.272892    0.41710031 -3.52050424  3.07600212 -6.55063343  4.62267637\n",
      " -3.52050424  3.07600212 -8.93501472  0.66584051  3.3787868   2.09500265\n",
      " -6.55063343  4.05369663  3.3787868   2.09500265 -6.36572361  0.8164807\n",
      " 25.69290733  1.11400259 -6.55063343  3.38661671 25.69290733  1.11400259]\n",
      "The state for the first agent looks like: [ -4.25472927   0.84913683 -30.           1.3102026    6.55063343\n",
      "   4.05369663 -30.           1.3102026   -1.25472999   0.92129707\n",
      "  30.           0.32920253   6.55063343   3.38661671  30.\n",
      "   0.32920253  -0.63986498   0.89535731  18.0067482   -0.65179747\n",
      "   6.55063343   2.62143707  18.0067482   -0.65179747]\n",
      "The state for the second agent looks like: [-8.93501472  0.66584051  3.3787868   2.09500265 -6.55063343  4.05369663\n",
      "  3.3787868   2.09500265 -6.36572361  0.8164807  25.69290733  1.11400259\n",
      " -6.55063343  3.38661671 25.69290733  1.11400259 -4.80272055  0.86902094\n",
      " 15.63002586  0.13300253 -6.55063343  2.62143707 15.63002586  0.13300253]\n",
      "The state for the first agent looks like: [-1.25472999  0.92129707 30.          0.32920253  6.55063343  3.38661671\n",
      " 30.          0.32920253 -0.63986498  0.89535731 18.0067482  -0.65179747\n",
      "  6.55063343  2.62143707 18.0067482  -0.65179747 -0.90126699  0.7713176\n",
      "  4.93664694 -1.63279748  6.55063343  1.75815713  4.93664694 -1.63279748]\n",
      "The state for the second agent looks like: [ -6.36572361   0.8164807   25.69290733   1.11400259  -6.55063343\n",
      "   3.38661671  25.69290733   1.11400259  -4.80272055   0.86902094\n",
      "  15.63002586   0.13300253  -6.55063343   2.62143707  15.63002586\n",
      "   0.13300253  -6.82316303   0.82346117 -20.20442009  -0.84799749\n",
      "  -6.55063343   1.75815713 -20.20442009  -0.84799749]\n",
      "The state for the first agent looks like: [ -0.63986498   0.89535731  18.0067482   -0.65179747   6.55063343\n",
      "   2.62143707  18.0067482   -0.65179747  -0.90126699   0.7713176\n",
      "   4.93664694  -1.63279748   6.55063343   1.75815713   4.93664694\n",
      "  -1.63279748  -3.99999976   0.54917783 -30.          -2.61379719\n",
      "   6.55063343   0.83853728 -30.          -2.61379719]\n",
      "The state for the second agent looks like: [ -4.80272055   0.86902094  15.63002586   0.13300253  -6.55063343\n",
      "   2.62143707  15.63002586   0.13300253  -6.82316303   0.82346117\n",
      " -20.20442009  -0.84799749  -6.55063343   1.75815713 -20.20442009\n",
      "  -0.84799749  -8.711586     0.67980146 -18.88422585  -1.82899749\n",
      "  -6.55063343   0.83853728 -18.88422585  -1.82899749]\n",
      "The state for the first agent looks like: [ -0.90126699   0.7713176    4.93664694  -1.63279748   6.55063343\n",
      "   1.75815713   4.93664694  -1.63279748  -3.99999976   0.54917783\n",
      " -30.          -2.61379719   6.55063343   0.83853728 -30.\n",
      "  -2.61379719  -1.00369811   0.22893813  29.96302032  -3.59479666\n",
      "   6.55063343  -0.08108279  29.96302032  -3.59479666]\n",
      "The state for the second agent looks like: [ -6.82316303   0.82346117 -20.20442009  -0.84799749  -6.55063343\n",
      "   1.75815713 -20.20442009  -0.84799749  -8.711586     0.67980146\n",
      " -18.88422585  -1.82899749  -6.55063343   0.83853728 -18.88422585\n",
      "  -1.82899749 -10.46942711   0.43804172 -17.57840157  -2.80999708\n",
      "  -6.55063343  -0.08108279 -17.57840157  -2.80999708]\n",
      "The state for the first agent looks like: [ -3.99999976   0.54917783 -30.          -2.61379719   6.55063343\n",
      "   0.83853728 -30.          -2.61379719  -1.00369811   0.22893813\n",
      "  29.96302032  -3.59479666   6.55063343  -0.08108279  29.96302032\n",
      "  -3.59479666  -0.55536371  -0.18940151  22.23181152  -4.57579613\n",
      "   6.55063343  -1.00070286  22.23181152  -4.57579613]\n",
      "The state for the second agent looks like: [ -8.711586     0.67980146 -18.88422585  -1.82899749  -6.55063343\n",
      "   0.83853728 -18.88422585  -1.82899749 -10.46942711   0.43804172\n",
      " -17.57840157  -2.80999708  -6.55063343  -0.08108279 -17.57840157\n",
      "  -2.80999708  -8.18610668   0.09818202  22.83320045  -3.79099655\n",
      "  -6.55063343  -1.00070286  22.83320045  -3.79099655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state for the first agent looks like: [ -1.00369811   0.22893813  29.96302032  -3.59479666   6.55063343\n",
      "  -0.08108279  29.96302032  -3.59479666  -0.55536371  -0.18940151\n",
      "  22.23181152  -4.57579613   6.55063343  -1.00070286  22.23181152\n",
      "  -4.57579613  -3.99999976  -0.70584118 -30.          -5.5567956\n",
      "   6.55063343  -1.92032266 -30.          -5.5567956 ]\n",
      "The state for the second agent looks like: [-10.46942711   0.43804172 -17.57840157  -2.80999708  -6.55063343\n",
      "  -0.08108279 -17.57840157  -2.80999708  -8.18610668   0.09818202\n",
      "  22.83320045  -3.79099655  -6.55063343  -1.00070286  22.83320045\n",
      "  -3.79099655  -9.44542885  -0.33977759 -12.59322834  -4.77199602\n",
      "  -6.55063343  -1.92032266 -12.59322834  -4.77199602]\n",
      "The state for the first agent looks like: [ -0.55536371  -0.18940151  22.23181152  -4.57579613   6.55063343\n",
      "  -1.00070286  22.23181152  -4.57579613  -3.99999976  -0.70584118\n",
      " -30.          -5.5567956    6.55063343  -1.92032266 -30.\n",
      "  -5.5567956   -4.15496922  -1.32038057  -1.54969716  -6.53779507\n",
      "   6.55063343  -2.83994246  -1.54969716  -6.53779507]\n",
      "The state for the second agent looks like: [ -8.18610668   0.09818202  22.83320045  -3.79099655  -6.55063343\n",
      "  -1.00070286  22.83320045  -3.79099655  -9.44542885  -0.33977759\n",
      " -12.59322834  -4.77199602  -6.55063343  -1.92032266 -12.59322834\n",
      "  -4.77199602 -10.91361523  -0.87583721   0.          -5.75299549\n",
      "  -6.55063343  -2.83994246   0.          -5.75299549]\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.14469624 -1.5\n",
      " -0.          0.          7.76153517  5.96076012 -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -7.55199575 -1.5\n",
      "  0.          0.         -7.76153517  5.96076012  0.          0.        ]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.61764145 -1.5        -0.          0.\n",
      "  7.76153517  5.94114017 -0.          0.         -3.61764216 -0.98316395\n",
      " 30.          6.21520042  7.76153517  5.78418016 30.          6.21520042]\n",
      "The state for the second agent looks like: [  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.          -6.997437    -1.5\n",
      "   0.           0.          -7.76153517   5.94114017   0.\n",
      "   0.          -9.99743843  -1.55886006 -30.          -0.98100001\n",
      "  -7.76153517   5.78418016 -30.          -0.98100001]\n",
      "The state for the first agent looks like: [-6.61764145 -1.5        -0.          0.          7.76153517  5.94114017\n",
      " -0.          0.         -3.61764216 -0.98316395 30.          6.21520042\n",
      "  7.76153517  5.78418016 30.          6.21520042 -3.43549037 -0.42050385\n",
      "  1.82151246  5.23420095  7.76153517  5.52912045  1.82151246  5.23420095]\n",
      "The state for the second agent looks like: [ -6.997437    -1.5          0.           0.          -7.76153517\n",
      "   5.94114017   0.           0.          -9.99743843  -1.55886006\n",
      " -30.          -0.98100001  -7.76153517   5.78418016 -30.\n",
      "  -0.98100001  -9.96959591  -0.91772008   0.27842879   6.01900053\n",
      "  -7.76153517   5.52912045   0.27842879   6.01900053]\n",
      "The state for the first agent looks like: [-3.61764216 -0.98316395 30.          6.21520042  7.76153517  5.78418016\n",
      " 30.          6.21520042 -3.43549037 -0.42050385  1.82151246  5.23420095\n",
      "  7.76153517  5.52912045  1.82151246  5.23420095 -2.99494767  0.0440563\n",
      "  4.40542746  4.25320148  7.76153517  5.17596054  4.40542746  4.25320148]\n",
      "The state for the second agent looks like: [ -9.99743843  -1.55886006 -30.          -0.98100001  -7.76153517\n",
      "   5.78418016 -30.          -0.98100001  -9.96959591  -0.91772008\n",
      "   0.27842879   6.01900053  -7.76153517   5.52912045   0.27842879\n",
      "   6.01900053  -7.61804008  -0.37467998  23.51554298   5.03800106\n",
      "  -7.76153517   5.17596054  23.51554298   5.03800106]\n",
      "The state for the first agent looks like: [-3.43549037 -0.42050385  1.82151246  5.23420095  7.76153517  5.52912045\n",
      "  1.82151246  5.23420095 -2.99494767  0.0440563   4.40542746  4.25320148\n",
      "  7.76153517  5.17596054  4.40542746  4.25320148 -0.39999998  0.41051644\n",
      " 30.          3.27220201  7.76153517  4.72470045 30.          3.27220201]\n",
      "The state for the second agent looks like: [-9.96959591 -0.91772008  0.27842879  6.01900053 -7.76153517  5.52912045\n",
      "  0.27842879  6.01900053 -7.61804008 -0.37467998 23.51554298  5.03800106\n",
      " -7.76153517  5.17596054 23.51554298  5.03800106 -4.61804056  0.07026011\n",
      " 30.          4.05700159 -7.76153517  4.72470045 30.          4.05700159]\n",
      "The state for the first agent looks like: [-2.99494767  0.0440563   4.40542746  4.25320148  7.76153517  5.17596054\n",
      "  4.40542746  4.25320148 -0.39999998  0.41051644 30.          3.27220201\n",
      "  7.76153517  4.72470045 30.          3.27220201 -0.9792847   0.67887664\n",
      "  1.03576112  2.29120255  7.76153517  4.17534065  1.03576112  2.29120255]\n",
      "The state for the second agent looks like: [ -7.61804008  -0.37467998  23.51554298   5.03800106  -7.76153517\n",
      "   5.17596054  23.51554298   5.03800106  -4.61804056   0.07026011\n",
      "  30.           4.05700159  -7.76153517   4.72470045  30.\n",
      "   4.05700159  -7.61804008   0.41710031 -30.           3.07600212\n",
      "  -7.76153517   4.17534065 -30.           3.07600212]\n",
      "The state for the first agent looks like: [ -0.39999998   0.41051644  30.           3.27220201   7.76153517\n",
      "   4.72470045  30.           3.27220201  -0.9792847    0.67887664\n",
      "   1.03576112   2.29120255   7.76153517   4.17534065   1.03576112\n",
      "   2.29120255  -3.99999976   0.84913683 -30.           1.3102026\n",
      "   7.76153517   3.52788067 -30.           1.3102026 ]\n",
      "The state for the second agent looks like: [ -4.61804056   0.07026011  30.           4.05700159  -7.76153517\n",
      "   4.72470045  30.           4.05700159  -7.61804008   0.41710031\n",
      " -30.           3.07600212  -7.76153517   4.17534065 -30.\n",
      "   3.07600212  -5.13274813   0.66584051  24.85292816   2.09500265\n",
      "  -7.76153517   3.52788067  24.85292816   2.09500265]\n",
      "The state for the first agent looks like: [ -0.9792847    0.67887664   1.03576112   2.29120255   7.76153517\n",
      "   4.17534065   1.03576112   2.29120255  -3.99999976   0.84913683\n",
      " -30.           1.3102026    7.76153517   3.52788067 -30.\n",
      "   1.3102026   -6.26421785   0.92129707 -22.64218521   0.32920253\n",
      "   7.76153517   2.78232098 -22.64218521   0.32920253]\n",
      "The state for the second agent looks like: [ -7.61804008   0.41710031 -30.           3.07600212  -7.76153517\n",
      "   4.17534065 -30.           3.07600212  -5.13274813   0.66584051\n",
      "  24.85292816   2.09500265  -7.76153517   3.52788067  24.85292816\n",
      "   2.09500265  -5.70885563   0.8164807   -5.76107979   1.11400259\n",
      "  -7.76153517   2.78232098  -5.76107979   1.11400259]\n",
      "The state for the first agent looks like: [ -3.99999976   0.84913683 -30.           1.3102026    7.76153517\n",
      "   3.52788067 -30.           1.3102026   -6.26421785   0.92129707\n",
      " -22.64218521   0.32920253   7.76153517   2.78232098 -22.64218521\n",
      "   0.32920253  -6.27256727   0.89535731  -0.08348835  -0.65179747\n",
      "   7.76153517   1.9386611   -0.08348835  -0.65179747]\n",
      "The state for the second agent looks like: [-5.13274813  0.66584051 24.85292816  2.09500265 -7.76153517  3.52788067\n",
      " 24.85292816  2.09500265 -5.70885563  0.8164807  -5.76107979  1.11400259\n",
      " -7.76153517  2.78232098 -5.76107979  1.11400259 -5.67320967  0.86902094\n",
      "  0.35646266  0.13300253 -7.76153517  1.9386611   0.35646266  0.13300253]\n",
      "The state for the first agent looks like: [ -6.26421785   0.92129707 -22.64218521   0.32920253   7.76153517\n",
      "   2.78232098 -22.64218521   0.32920253  -6.27256727   0.89535731\n",
      "  -0.08348835  -0.65179747   7.76153517   1.9386611   -0.08348835\n",
      "  -0.65179747  -7.81532621   0.7713176  -15.42758846  -1.63279748\n",
      "   8.83045197   2.37101722 -15.42758846  -1.63279748]\n",
      "The state for the second agent looks like: [ -5.70885563   0.8164807   -5.76107979   1.11400259  -7.76153517\n",
      "   2.78232098  -5.76107979   1.11400259  -5.67320967   0.86902094\n",
      "   0.35646266   0.13300253  -7.76153517   1.9386611    0.35646266\n",
      "   0.13300253  -8.54028893   0.467691   -30.          -6.7774992\n",
      "  -8.83045197   2.37101722 -30.          -6.7774992 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state for the first agent looks like: [ -6.27256727   0.89535731  -0.08348835  -0.65179747   7.76153517\n",
      "   1.9386611   -0.08348835  -0.65179747  -7.81532621   0.7713176\n",
      " -15.42758846  -1.63279748   8.83045197   2.37101722 -15.42758846\n",
      "  -1.63279748  -8.12854671   0.54917783  -3.13220191  -2.61379719\n",
      "   9.73045349   3.17291689  -3.13220191  -2.61379719]\n",
      "The state for the second agent looks like: [ -5.67320967   0.86902094   0.35646266   0.13300253  -7.76153517\n",
      "   1.9386611    0.35646266   0.13300253  -8.54028893   0.467691\n",
      " -30.          -6.7774992   -8.83045197   2.37101722 -30.\n",
      "  -6.7774992   -5.54028845  -0.26891881  30.          -7.75849867\n",
      "  -9.73045349   3.17291689  30.          -7.75849867]\n",
      "The state for the first agent looks like: [ -7.81532621   0.7713176  -15.42758846  -1.63279748   8.83045197\n",
      "   2.37101722 -15.42758846  -1.63279748  -8.12854671   0.54917783\n",
      "  -3.13220191  -2.61379719   9.73045349   3.17291689  -3.13220191\n",
      "  -2.61379719  -5.40066338   0.22893813  27.27882385  -3.59479666\n",
      "  10.63045502   3.87671685  27.27882385  -3.59479666]\n",
      "The state for the second agent looks like: [ -8.54028893   0.467691   -30.          -6.7774992   -8.83045197\n",
      "   2.37101722 -30.          -6.7774992   -5.54028845  -0.26891881\n",
      "  30.          -7.75849867  -9.73045349   3.17291689  30.\n",
      "  -7.75849867  -8.54028893  -1.10362875 -30.          -8.73950005\n",
      " -10.63045502   3.87671685 -30.          -8.73950005]\n",
      "The state for the first agent looks like: [-8.12854671  0.54917783 -3.13220191 -2.61379719  9.73045349  3.17291689\n",
      " -3.13220191 -2.61379719 -5.40066338  0.22893813 27.27882385 -3.59479666\n",
      " 10.63045502  3.87671685 27.27882385 -3.59479666 -5.64107275 -0.18940151\n",
      " -2.40409493 -4.57579613 11.53045654  4.48241711 -2.40409493 -4.57579613]\n",
      "The state for the second agent looks like: [ -5.54028845  -0.26891881  30.          -7.75849867  -9.73045349\n",
      "   3.17291689  30.          -7.75849867  -8.54028893  -1.10362875\n",
      " -30.          -8.73950005 -10.63045502   3.87671685 -30.\n",
      "  -8.73950005  -6.60163546  -1.37939084  19.38652611   6.80380011\n",
      " -11.53045654   4.48241711  19.38652611   6.80380011]\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.37251186 -1.5\n",
      " -0.          0.          6.04292488  5.98822832 -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.46944427 -1.5\n",
      "  0.          0.         -6.04292488  5.98822832  0.          0.        ]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.66271734 -1.5        -0.          0.\n",
      "  6.04292488  5.97645617 -0.          0.         -7.06513453 -1.55886006\n",
      " -4.02418041 -0.98100001  6.04292488  5.85873604 -4.02418041 -0.98100001]\n",
      "The state for the second agent looks like: [  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.          -7.9240942   -1.5\n",
      "   0.           0.          -6.04292488   5.97645617   0.\n",
      "   0.         -10.92409611  -0.98316395 -30.           6.21520042\n",
      "  -6.04292488   5.85873604 -30.           6.21520042]\n",
      "The state for the first agent looks like: [ -6.66271734  -1.5         -0.           0.           6.04292488\n",
      "   5.97645617  -0.           0.          -7.06513453  -1.55886006\n",
      "  -4.02418041  -0.98100001   6.04292488   5.85873604  -4.02418041\n",
      "  -0.98100001  -9.70607948  -1.71581995 -26.40945435  -1.96200001\n",
      "   6.04292488   5.6429162  -26.40945435  -1.96200001]\n",
      "The state for the second agent looks like: [ -7.9240942   -1.5          0.           0.          -6.04292488\n",
      "   5.97645617   0.           0.         -10.92409611  -0.98316395\n",
      " -30.           6.21520042  -6.04292488   5.85873604 -30.\n",
      "   6.21520042 -10.41691303  -0.42050385   5.07183695   5.23420095\n",
      "  -6.04292488   5.6429162    5.07183695   5.23420095]\n",
      "The state for the first agent looks like: [ -7.06513453  -1.55886006  -4.02418041  -0.98100001   6.04292488\n",
      "   5.85873604  -4.02418041  -0.98100001  -9.70607948  -1.71581995\n",
      " -26.40945435  -1.96200001   6.04292488   5.6429162  -26.40945435\n",
      "  -1.96200001  -9.59144306  -1.05898392   1.14636171   6.21520042\n",
      "   6.04292488   5.32899618   1.14636171   6.21520042]\n",
      "The state for the second agent looks like: [-10.92409611  -0.98316395 -30.           6.21520042  -6.04292488\n",
      "   5.85873604 -30.           6.21520042 -10.41691303  -0.42050385\n",
      "   5.07183695   5.23420095  -6.04292488   5.6429162    5.07183695\n",
      "   5.23420095 -10.89986038   0.0440563    0.           4.25320148\n",
      "  -6.04292488   5.32899618   0.           4.25320148]\n",
      "The state for the first agent looks like: [ -9.70607948  -1.71581995 -26.40945435  -1.96200001   6.04292488\n",
      "   5.6429162  -26.40945435  -1.96200001  -9.59144306  -1.05898392\n",
      "   1.14636171   6.21520042   6.04292488   5.32899618   1.14636171\n",
      "   6.21520042  -7.8154006   -0.49632382  17.7604084    5.23420095\n",
      "   6.04292488   4.91697645  17.7604084    5.23420095]\n",
      "The state for the second agent looks like: [-10.41691303  -0.42050385   5.07183695   5.23420095  -6.04292488\n",
      "   5.6429162    5.07183695   5.23420095 -10.89986038   0.0440563\n",
      "   0.           4.25320148  -6.04292488   5.32899618   0.\n",
      "   4.25320148  -7.94514799   0.41051644  29.54711533   3.27220201\n",
      "  -6.04292488   4.91697645  29.54711533   3.27220201]\n",
      "The state for the first agent looks like: [ -9.59144306  -1.05898392   1.14636171   6.21520042   6.04292488\n",
      "   5.32899618   1.14636171   6.21520042  -7.8154006   -0.49632382\n",
      "  17.7604084    5.23420095   6.04292488   4.91697645  17.7604084\n",
      "   5.23420095 -10.81540298  -0.0317637  -30.           4.25320148\n",
      "   6.04292488   4.40685654 -30.           4.25320148]\n",
      "The state for the second agent looks like: [-10.89986038   0.0440563    0.           4.25320148  -6.04292488\n",
      "   5.32899618   0.           4.25320148  -7.94514799   0.41051644\n",
      "  29.54711533   3.27220201  -6.04292488   4.91697645  29.54711533\n",
      "   3.27220201  -4.94514847   0.67887664  30.           2.29120255\n",
      "  -6.04292488   4.40685654  30.           2.29120255]\n",
      "The state for the first agent looks like: [ -7.8154006   -0.49632382  17.7604084    5.23420095   6.04292488\n",
      "   4.91697645  17.7604084    5.23420095 -10.81540298  -0.0317637\n",
      " -30.           4.25320148   6.04292488   4.40685654 -30.\n",
      "   4.25320148 -10.90061378   0.33469647  -0.           3.27220201\n",
      "   6.04292488   3.79863667  -0.           3.27220201]\n",
      "The state for the second agent looks like: [ -7.94514799   0.41051644  29.54711533   3.27220201  -6.04292488\n",
      "   4.91697645  29.54711533   3.27220201  -4.94514847   0.67887664\n",
      "  30.           2.29120255  -6.04292488   4.40685654  30.\n",
      "   2.29120255  -7.94514799   0.84913683 -30.           1.3102026\n",
      "  -6.04292488   3.79863667 -30.           1.3102026 ]\n",
      "The state for the first agent looks like: [-10.81540298  -0.0317637  -30.           4.25320148   6.04292488\n",
      "   4.40685654 -30.           4.25320148 -10.90061378   0.33469647\n",
      "  -0.           3.27220201   6.04292488   3.79863667  -0.\n",
      "   3.27220201 -10.89978981   0.60305667  -0.           2.29120255\n",
      "   6.04292488   3.09231687  -0.           2.29120255]\n",
      "The state for the second agent looks like: [ -4.94514847   0.67887664  30.           2.29120255  -6.04292488\n",
      "   4.40685654  30.           2.29120255  -7.94514799   0.84913683\n",
      " -30.           1.3102026   -6.04292488   3.79863667 -30.\n",
      "   1.3102026   -9.49601841   0.92129707 -15.5086813    0.32920253\n",
      "  -6.04292488   3.09231687 -15.5086813    0.32920253]\n",
      "The state for the first agent looks like: [-10.90061378   0.33469647  -0.           3.27220201   6.04292488\n",
      "   3.79863667  -0.           3.27220201 -10.89978981   0.60305667\n",
      "  -0.           2.29120255   6.04292488   3.09231687  -0.\n",
      "   2.29120255  -7.89978743   0.77331686  30.           1.3102026\n",
      "   6.04292488   2.28789711  30.           1.3102026 ]\n",
      "The state for the second agent looks like: [ -7.94514799   0.84913683 -30.           1.3102026   -6.04292488\n",
      "   3.79863667 -30.           1.3102026   -9.49601841   0.92129707\n",
      " -15.5086813    0.32920253  -6.04292488   3.09231687 -15.5086813\n",
      "   0.32920253  -7.72637987   0.89535731  17.69638062  -0.65179747\n",
      "  -6.04292488   2.28789711  17.69638062  -0.65179747]\n",
      "The state for the first agent looks like: [-10.89978981   0.60305667  -0.           2.29120255   6.04292488\n",
      "   3.09231687  -0.           2.29120255  -7.89978743   0.77331686\n",
      "  30.           1.3102026    6.04292488   2.28789711  30.\n",
      "   1.3102026   -8.59142113   0.8454771   -6.91635084   0.32920253\n",
      "   6.04292488   1.39030921  -6.91635084   0.32920253]\n",
      "The state for the second agent looks like: [ -9.49601841   0.92129707 -15.5086813    0.32920253  -6.04292488\n",
      "   3.09231687 -15.5086813    0.32920253  -7.72637987   0.89535731\n",
      "  17.69638062  -0.65179747  -6.04292488   2.28789711  17.69638062\n",
      "  -0.65179747  -7.0077529    0.7713176    7.1862669   -1.63279748\n",
      "  -6.04292488   1.39030921   7.1862669   -1.63279748]\n",
      "The state for the first agent looks like: [-7.89978743  0.77331686 30.          1.3102026   6.04292488  2.28789711\n",
      " 30.          1.3102026  -8.59142113  0.8454771  -6.91635084  0.32920253\n",
      "  6.04292488  1.39030921 -6.91635084  0.32920253 -6.94777727  0.81953728\n",
      " 16.43642807 -0.65179747  6.04292488  0.47068924 16.43642807 -0.65179747]\n",
      "The state for the second agent looks like: [-7.72637987  0.89535731 17.69638062 -0.65179747 -6.04292488  2.28789711\n",
      " 17.69638062 -0.65179747 -7.0077529   0.7713176   7.1862669  -1.63279748\n",
      " -6.04292488  1.39030921  7.1862669  -1.63279748 -7.27618837  0.54917783\n",
      " -2.68435335 -2.61379719 -6.04292488  0.47068924 -2.68435335 -2.61379719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state for the first agent looks like: [ -8.59142113   0.8454771   -6.91635084   0.32920253   6.04292488\n",
      "   1.39030921  -6.91635084   0.32920253  -6.94777727   0.81953728\n",
      "  16.43642807  -0.65179747   6.04292488   0.47068924  16.43642807\n",
      "  -0.65179747  -9.9477787    0.69549763 -30.          -1.63279748\n",
      "   6.04292488  -0.4489308  -30.          -1.63279748]\n",
      "The state for the second agent looks like: [ -7.0077529    0.7713176    7.1862669   -1.63279748  -6.04292488\n",
      "   1.39030921   7.1862669   -1.63279748  -7.27618837   0.54917783\n",
      "  -2.68435335  -2.61379719  -6.04292488   0.47068924  -2.68435335\n",
      "  -2.61379719 -10.24077606   0.22893813 -29.64585304  -3.59479666\n",
      "  -6.04292488  -0.4489308  -29.64585304  -3.59479666]\n",
      "The state for the first agent looks like: [ -6.94777727   0.81953728  16.43642807  -0.65179747   6.04292488\n",
      "   0.47068924  16.43642807  -0.65179747  -9.9477787    0.69549763\n",
      " -30.          -1.63279748   6.04292488  -0.4489308  -30.\n",
      "  -1.63279748  -9.31042385   0.47335786   6.37356472  -2.61379719\n",
      "   6.04292488  -1.36855078   6.37356472  -2.61379719]\n",
      "The state for the second agent looks like: [ -7.27618837   0.54917783  -2.68435335  -2.61379719  -6.04292488\n",
      "   0.47068924  -2.68435335  -2.61379719 -10.24077606   0.22893813\n",
      " -29.64585304  -3.59479666  -6.04292488  -0.4489308  -29.64585304\n",
      "  -3.59479666  -9.59795189  -0.18940151   6.42823553  -4.57579613\n",
      "  -6.04292488  -1.36855078   6.42823553  -4.57579613]\n",
      "The state for the first agent looks like: [ -9.9477787    0.69549763 -30.          -1.63279748   6.04292488\n",
      "  -0.4489308  -30.          -1.63279748  -9.31042385   0.47335786\n",
      "   6.37356472  -2.61379719   6.04292488  -1.36855078   6.37356472\n",
      "  -2.61379719  -9.15933323   0.15311816   1.51091254  -3.59479666\n",
      "   6.04292488  -2.28817058   1.51091254  -3.59479666]\n",
      "The state for the second agent looks like: [-10.24077606   0.22893813 -29.64585304  -3.59479666  -6.04292488\n",
      "  -0.4489308  -29.64585304  -3.59479666  -9.59795189  -0.18940151\n",
      "   6.42823553  -4.57579613  -6.04292488  -1.36855078   6.42823553\n",
      "  -4.57579613 -10.91971684  -0.70584118   0.          -5.5567956\n",
      "  -6.04292488  -2.28817058   0.          -5.5567956 ]\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.14030886 -1.5\n",
      " -0.          0.         -7.11741829  5.99607611 -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -7.9574213  -1.5\n",
      "  0.          0.          7.11741829  5.99607611  0.          0.        ]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        \n",
    "        print('The state for the first agent looks like:', states[0])\n",
    "        print('The state for the second agent looks like:', states[1])\n",
    "        \n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main code taken from Udacity's repository:\n",
    "##https://github.com/udacity/deep-reinforcement-learning/\n",
    "##\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##Hidden layer's weights intialization\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        _input_layer = 128#128\n",
    "        _hidden_1 = 128#256\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, _input_layer)\n",
    "        self.fc2 = nn.Linear(_input_layer, _hidden_1)\n",
    "        self.fc3 = nn.Linear(_hidden_1, action_size)\n",
    "        \n",
    "        #batchnorm\n",
    "        self.bn_input = nn.BatchNorm1d(_input_layer)\n",
    "        self.bn_hidden = nn.BatchNorm1d(_hidden_1)\n",
    "\n",
    "        #Dropout\n",
    "        self.dpout = nn.Dropout(p=0.20)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        #If there are multiple windows, the state will have them already concatenated\n",
    "        #Input layer: Dense + Batchnorm + ReLU\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.fc1(state) \n",
    "        x = self.bn_input(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #First hidden layer: Dense + ReLU\n",
    "        x = self.fc2(x)\n",
    "#         x = self.bn_hidden(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #Output layer: Dense  + Tanh\n",
    "        x = self.fc3(x)\n",
    "        out = torch.tanh(x)\n",
    "        #norm = torch.linalg.vector_norm(x)#torch.norm(h3)\n",
    "            \n",
    "        #return 10.0*(f.tanh(norm))*x/norm if norm > 0 else 10*x\n",
    "\n",
    "        return out\n",
    "        \"\"\"\n",
    "    \n",
    "        x = F.relu(self.fc1(state))\n",
    "        \n",
    "        x = self.bn_input(x) \n",
    "            \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        _input_layer = 128#128\n",
    "        _hidden_1 = 128#256\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fcs1 = nn.Linear(state_size, _input_layer)\n",
    "        self.fc2 = nn.Linear(_input_layer + action_size, _hidden_1) #concat the action from the actor\n",
    "        self.fc3 = nn.Linear(_hidden_1, 1)\n",
    "        \n",
    "        #batchnorm\n",
    "        self.bn_input = nn.BatchNorm1d(_input_layer)\n",
    "        \n",
    "        #Dropout\n",
    "        self.dpout = nn.Dropout(p=0.20)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        #If there are multiple windows, the state will have them already concatenated\n",
    "        #Input layer: Dense + Batchnorm + ReLU\n",
    "        \"\"\"\n",
    "        x = self.fcs1(state) \n",
    "        x = self.bn_input(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #First hidden layer Dense + ReLU\n",
    "        #Concat state & action\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #Output layer: Dense\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "        \"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        xs = self.bn_input(xs) \n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_in_dim, hidden_out_dim, output_dim, actor=False):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        \"\"\"self.input_norm = nn.BatchNorm1d(input_dim)\n",
    "        self.input_norm.weight.data.fill_(1)\n",
    "        self.input_norm.bias.data.fill_(0)\"\"\"\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_in_dim)\n",
    "        self.fc2 = nn.Linear(hidden_in_dim,hidden_out_dim)\n",
    "        self.fc3 = nn.Linear(hidden_out_dim,output_dim)\n",
    "        self.nonlin = f.relu #leaky_relu\n",
    "        self.actor = actor\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.actor:\n",
    "            # return a vector of the force\n",
    "            h1 = self.nonlin(self.fc1(x))\n",
    "\n",
    "            h2 = self.nonlin(self.fc2(h1))\n",
    "            h3 = (self.fc3(h2))\n",
    "            #norm = torch.norm(h3)\n",
    "            norm = torch.linalg.vector_norm(h3)#torch.norm(h3)\n",
    "            \n",
    "            # h3 is a 2D vector (a force that is applied to the agent)\n",
    "            # we bound the norm of the vector to be between 0 and 10\n",
    "            return 10.0*(f.tanh(norm))*h3/norm if norm > 0 else 10*h3\n",
    "            \n",
    "            # h3 is a 2D vector (a force that is applied to the agent)\n",
    "            # we bound the norm of the vector to be between 0 and 10\n",
    "            #return 10.0*(f.tanh(norm))*h3/norm if norm > 0 else 10*h3\n",
    "        \n",
    "        else:\n",
    "            # critic network simply outputs a number\n",
    "            h1 = self.nonlin(self.fc1(x))\n",
    "            h2 = self.nonlin(self.fc2(h1))\n",
    "            h3 = (self.fc3(h2))\n",
    "            return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# from https://github.com/songrotek/DDPG/blob/master/ou_noise.py\n",
    "class OUNoise:\n",
    "\n",
    "    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.scale = scale\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state * self.scale#torch.tensor(self.state * self.scale).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 34: CUDA driver is a stub library (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# individual network settings for each actor + critic pair\n",
    "# see networkforall for details\n",
    "\n",
    "#from networkforall import Network\n",
    "#from utilities import hard_update, gumbel_softmax, onehot_from_logits\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# add OU noise for exploration\n",
    "#from OUNoise import OUNoise\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    \"\"\"\n",
    "    Copy network parameters from source to target\n",
    "    Inputs:\n",
    "        target (torch.nn.Module): Net to copy parameters to\n",
    "        source (torch.nn.Module): Net whose parameters to copy\n",
    "    \"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    #def __init__(self, state_size, action_size, lr_actor=1e-4, lr_critic=1e-3):#, lr_actor=1.0e-2, lr_critic=1.0e-2):\n",
    "    #def __init__(self, in_actor, hidden_in_actor, hidden_out_actor, out_actor, in_critic, hidden_in_critic, hidden_out_critic, lr_actor=1.0e-2, lr_critic=1.0e-2): \n",
    "    def __init__(self, in_actor, out_actor, criticStateSize, criticActionSize, lr_actor=1.0e-2, lr_critic=1.0e-2): \n",
    "        super(DDPGAgent, self).__init__()\n",
    "        \n",
    "        \n",
    "        seed = 2\n",
    "        #Create Actor networks\n",
    "        self.actor = Actor(in_actor, out_actor, seed).to(device)\n",
    "        self.target_actor = Actor(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        #Create Critic networks\n",
    "        #criticStateSize = 2*state_size\n",
    "        #criticActionSize = 2*action_size\n",
    "        self.critic = Critic(criticStateSize, criticActionSize, seed).to(device)        \n",
    "        self.target_critic = Critic(criticStateSize, criticActionSize, seed).to(device)\n",
    "\n",
    "        self.noise = OUNoise(action_size, scale=1.0 )\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.actor = Network(in_actor, hidden_in_actor, hidden_out_actor, out_actor, actor=True).to(device)\n",
    "        self.critic = Network(in_critic, hidden_in_critic, hidden_out_critic, 1).to(device)\n",
    "        self.target_actor = Network(in_actor, hidden_in_actor, hidden_out_actor, out_actor, actor=True).to(device)\n",
    "        self.target_critic = Network(in_critic, hidden_in_critic, hidden_out_critic, 1).to(device)\n",
    "        \"\"\"\n",
    "        self.noise = OUNoise(out_actor, scale=1.0 )\n",
    "\n",
    "        \n",
    "        # initialize targets same as original networks\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "        \n",
    "        #Create Actor and Critic optimizers\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=lr_critic, weight_decay=1.e-5)\n",
    "\n",
    "\n",
    "    def act(self, obs, noise=0.0):\n",
    "        \n",
    "        if torch.is_tensor(obs):\n",
    "            obs = obs.to(device)\n",
    "        else:\n",
    "            list_of_tensors = [torch.from_numpy(arr).float().to(device) if not torch.is_tensor(arr) \\\n",
    "                               else arr for arr in obs]\n",
    "            if not torch.is_tensor(list_of_tensors):\n",
    "                obs = torch.stack(list_of_tensors)\n",
    "            #for i in range(len(obs)):\n",
    "            #    obs[i] = torch.from_numpy(obs[i]).float().to(device)\n",
    "            #obs = torch.from_numpy(obs).float().to(device)\n",
    "        #obs = torch.from_numpy(obs).float().to(device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(obs).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        \n",
    "        #if add_noise:            \n",
    "        action += noise*self.noise.noise()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "        #action = self.actor(obs) + noise*self.noise.noise()\n",
    "        #return action\n",
    "\n",
    "\n",
    "    def target_act(self, obs, noise=0.0):\n",
    "        #obs = obs.to(device)\n",
    "        #action = self.target_actor(obs) + noise*self.noise.noise()\n",
    "        #return action\n",
    "        \n",
    "        if torch.is_tensor(obs):\n",
    "            obs = obs.to(device)\n",
    "        else:\n",
    "            #list_of_tensors = [torch.from_numpy(arr).float().to(device) for arr in obs]\n",
    "            #obs = torch.stack(list_of_tensors)\n",
    "            \n",
    "            list_of_tensors = [torch.from_numpy(arr).float().to(device) if not torch.is_tensor(arr) \\\n",
    "                               else arr for arr in obs]\n",
    "            if not torch.is_tensor(list_of_tensors):\n",
    "                obs = torch.stack(list_of_tensors)\n",
    "                \n",
    "            #for i in range(len(obs)):\n",
    "            #    obs[i] = torch.from_numpy(obs[i]).float().to(device)\n",
    "            #obs = torch.from_numpy(obs).float().to(device)\n",
    "        \n",
    "        #obs = torch.from_numpy(obs).float().to(device)\n",
    "        self.target_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.target_actor(obs).cpu().data.numpy()\n",
    "        self.target_actor.train()\n",
    "        \n",
    "        #if add_noise:            \n",
    "        action += noise*self.noise.noise()\n",
    "        return torch.tensor(np.clip(action, -1, 1))\n",
    "        \n",
    "        \"\"\"\n",
    "        obs = obs.to(device)\n",
    "        action = self.target_actor(obs) + torch.tensor(noise*self.noise.noise()).float() #noise*self.noise.noise()\n",
    "        return action\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main code that contains the neural network setup\n",
    "# policy + critic updates\n",
    "# see ddpg.py for other details in the network\n",
    "\n",
    "import random\n",
    "#import torch\n",
    "#from utilities import soft_update, transpose_to_tensor, transpose_list\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "def transpose_list(mylist):\n",
    "    return list(map(list, zip(*mylist)))\n",
    "\n",
    "def transpose_to_tensor(input_list):\n",
    "    make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n",
    "    return list(map(make_tensor, zip(*input_list)))\n",
    "\n",
    "\n",
    "\n",
    "class MADDPG:\n",
    "    \n",
    "    \n",
    "    def __init__(self, sharedActor=False, sharedCritic=False, discount_factor=0.95, tau=0.02):\n",
    "        super(MADDPG, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        self.maddpg_agent = [DDPGAgent(24, 32, 16, 2, 48, 64, 32), #DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                             DDPGAgent(24, 32, 16, 2, 48, 64, 32)]\n",
    "        \n",
    "        self.maddpg_agent = [DDPGAgent(24, 32, 16, 2, 52, 64, 32), #DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                             DDPGAgent(24, 32, 16, 2, 52, 64, 32)]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.maddpg_agent = [DDPGAgent(24, 2, 48, 4), #DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                             DDPGAgent(24, 2, 48, 4)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.discount_factor = 1 #No discount for this project #discount_factor\n",
    "        self.tau = tau\n",
    "        self.iter = 0\n",
    "        self.sharedActor = sharedActor\n",
    "        self.sharedCritic = sharedCritic\n",
    "        \n",
    "        if sharedActor:\n",
    "            #merge the actor networks\n",
    "            for ddpgAgent in self.maddpg_agent:\n",
    "                ddpgAgent.actor = self.maddpg_agent[0].actor\n",
    "                ddpgAgent.target_actor = self.maddpg_agent[0].target_actor\n",
    "                \n",
    "        if sharedCritic:\n",
    "            #merge the critic networks\n",
    "            for ddpgAgent in self.maddpg_agent:\n",
    "                ddpgAgent.critic = self.maddpg_agent[0].critic\n",
    "                ddpgAgent.target_critic = self.maddpg_agent[0].target_critic\n",
    "                \n",
    "        # Replay memory\n",
    "        #self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "\n",
    "    #def createReplayBuffer(self, action_size, BUFFER_SIZE, BATCH_SIZE, random_Seed):\n",
    "    # Replay memory\n",
    "    #self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    #def getReplayBuffer\n",
    "    #def push(self, states, actions, rewards, next_States, dones):\n",
    "        \n",
    "    \n",
    "    def get_actors(self):\n",
    "        \"\"\"get actors of all the agents in the MADDPG object\"\"\"\n",
    "        actors = [ddpg_agent.actor for ddpg_agent in self.maddpg_agent]\n",
    "        return actors\n",
    "\n",
    "    def get_target_actors(self):\n",
    "        \"\"\"get target_actors of all the agents in the MADDPG object\"\"\"\n",
    "        target_actors = [ddpg_agent.target_actor for ddpg_agent in self.maddpg_agent]\n",
    "        return target_actors\n",
    "\n",
    "    def act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        #actions = [agent.act(obs, noise) for agent, obs in zip(self.maddpg_agent, obs_all_agents)]\n",
    "        \n",
    "        actions = None\n",
    "        if self.sharedActor:\n",
    "            actions = self.maddpg_agent[0].act(obs_all_agents, noise)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def target_act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        #target_actions = [ddpg_agent.target_act(obs, noise) for ddpg_agent, obs in zip(self.maddpg_agent, obs_all_agents)]\n",
    "        \n",
    "        target_actions = None\n",
    "        if self.sharedActor:\n",
    "            #this will be a tensor of tensor. We have to turn it into a list of tensors\n",
    "            target_actions = self.maddpg_agent[0].target_act(obs_all_agents, noise)\n",
    "            target_actions = [tens for tens in target_actions]\n",
    "        return target_actions\n",
    "\n",
    "    def update(self, samples, agent_number):\n",
    "        \"\"\"update the critics and actors of all the agents \"\"\"\n",
    "\n",
    "        # need to transpose each element of the samples\n",
    "        # to flip obs[parallel_agent][agent_number] to\n",
    "        # obs[agent_number][parallel_agent]\n",
    "        #obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensor, samples)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = map(transpose_to_tensor, samples)\n",
    "        \n",
    "        #states = torch.stack(states)\n",
    "        #states = states.to(device)\n",
    "        states[0] = states[0].to(device)\n",
    "        states[1] = states[1].to(device)\n",
    "        #actions = torch.stack(actions)\n",
    "        #actions = actions.to(device)\n",
    "        actions[0] = actions[0].to(device)\n",
    "        actions[1] = actions[1].to(device)\n",
    "        rewards = torch.stack(rewards)\n",
    "        rewards = rewards.to(device)\n",
    "        #next_states = torch.stack(next_states)\n",
    "        #next_states = next_states.to(device)\n",
    "        next_states[0] = next_states[0].to(device)\n",
    "        next_states[1] = next_states[1].to(device)\n",
    "        dones = torch.stack(dones)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        \n",
    "        #print(\"********\")\n",
    "        #print(obs_full)\n",
    "        #obs_full = torch.stack(obs_full) #turns the list of tensoer into a tensor of tensors\n",
    "        states_full = torch.cat(states, dim=1)\n",
    "        states_full = states_full.to(device)\n",
    "        #print(obs_full)\n",
    "        #print(\"********\")\n",
    "        #next_obs_full = torch.stack(next_obs_full) ##turns the list of tensoer into a tensor of tensors\n",
    "        #print(len(next_states)) #3\n",
    "        #print(next_states[0].size())\n",
    "        next_states_full = torch.cat(next_states, dim=1)\n",
    "        next_states_full = next_states_full.to(device)\n",
    "        #print(next_states_full.size()) \n",
    "        agent = self.maddpg_agent[agent_number]\n",
    "        agent.critic_optimizer.zero_grad()\n",
    "\n",
    "        #critic loss = batch mean of (y- Q(s,a) from target network)^2 //perhaps it's Q(s,a) from critic network\n",
    "        #y = reward of this timestep + discount * Q(st+1,at+1c) from target network\n",
    "        #While training the current agent, we use the actions and observations for all the agents to predict the next actions,  we use the full observation for all the agents along with the predicted next actions to predict the q values, but we use the reward and done values of the current agent being trained.\n",
    "        #Based on the formula of y, we understand that values from critic networks are Q values and they are like cumulative rewards. That's why we try to maximize the critic values in the loss formula when training actor network (see below)\n",
    "\n",
    "        #print(\"tensor next_obs size:\")\n",
    "        #print(len(next_obs)) #3\n",
    "        #print(next_obs[0].size()) #torch.Size([1000, 14])\n",
    "        target_actions = self.target_act(next_states)\n",
    "        #print(\"tensor actions size before cat:\")\n",
    "        #print(len(target_actions)) #3\n",
    "        #print(target_actions.size())\n",
    "        #print(target_actions[0].size()) #torch.Size([1000, 2])\n",
    "        \n",
    "        target_actions = torch.cat(target_actions, dim=1) # we do a cat of 3 tensors of size (1000,2) we'll obtai a tesor of size (1000,6)\n",
    "        target_actions = target_actions.to(device)\n",
    "        #print(\"tensor observations size after transpo:\")\n",
    "        #print(next_obs_full.t().size()) #torch.Size([1000, 14])\n",
    "        #print(\"tensor actions size after cat:\")\n",
    "        #print(target_actions.size()) #torch.Size([1000, 6])\n",
    "        #target_critic_input = torch.cat((next_states_full.t(),target_actions), dim=1).to(device)\n",
    "        target_critic_input = torch.cat((next_states_full,target_actions), dim=1).to(device)\n",
    "        #print(\" target critic input aftercat:\")\n",
    "        #print(target_critic_input.size()) #torch.Size([1000, 20])\n",
    "        with torch.no_grad():\n",
    "            #q_next = agent.target_critic(target_critic_input)\n",
    "            q_next = agent.target_critic(next_states_full,target_actions)\n",
    "        \n",
    "        y = rewards[agent_number].view(-1, 1) + self.discount_factor * q_next * (1 - dones[agent_number].view(-1, 1))\n",
    "        actions = torch.cat(actions, dim=1)\n",
    "        #critic_input = torch.cat((states_full.t(), action), dim=1).to(device)\n",
    "        critic_input = torch.cat((states_full, actions), dim=1).to(device)\n",
    "        #q = agent.critic(critic_input)\n",
    "        q = agent.critic(states_full, actions)\n",
    "\n",
    "        huber_loss = torch.nn.SmoothL1Loss()\n",
    "        critic_loss = huber_loss(q, y.detach())\n",
    "        critic_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), 0.5)\n",
    "        agent.critic_optimizer.step()\n",
    "\n",
    "        #update actor network using policy gradient\n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        # make input to agent\n",
    "        # detach the other agents to save computation\n",
    "        # saves some time for computing derivative\n",
    "        \"\"\"\n",
    "        q_input = [ self.maddpg_agent[i].actor(ob) if i == agent_number \\\n",
    "                   else self.maddpg_agent[i].actor(ob).detach()\n",
    "                   for i, ob in enumerate(obs) ]\n",
    "        \"\"\"\n",
    "        \n",
    "        #states[0] = states[0].to(device)\n",
    "        #states[1] = states[1].to(device)\n",
    "        \n",
    "        q_input = [ self.maddpg_agent[i].actor(ob) for i, ob in enumerate(states) ]\n",
    "                \n",
    "        q_input = torch.cat(q_input, dim=1)\n",
    "        # combine all the actions and observations for input to critic\n",
    "        # many of the obs are redundant, and obs[1] contains all useful information already #perhaps it is obs[i]\n",
    "        \n",
    "        #states_full = states_full.to(device)\n",
    "        \n",
    "        q_input2 = torch.cat((states_full, q_input), dim=1)\n",
    "        \n",
    "        # get the policy gradient\n",
    "        actor_loss = -agent.critic(states_full, q_input).mean()#-agent.critic(q_input2).mean()\n",
    "        actor_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.5)\n",
    "        agent.actor_optimizer.step()\n",
    "\n",
    "        #al = actor_loss.cpu().detach().item()\n",
    "        #cl = critic_loss.cpu().detach().item()\n",
    "\n",
    "    def update_targets(self):\n",
    "        \"\"\"soft update targets\"\"\"\n",
    "        self.iter += 1\n",
    "        for ddpg_agent in self.maddpg_agent:\n",
    "            soft_update(ddpg_agent.target_actor, ddpg_agent.actor, self.tau)\n",
    "            soft_update(ddpg_agent.target_critic, ddpg_agent.critic, self.tau)\n",
    "                    \n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.deque = deque(maxlen=self.size)\n",
    "\n",
    "    def push(self,transition):\n",
    "        \"\"\"push into the buffer\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        input_to_buffer = transpose_list(transition)\n",
    "    \n",
    "        for item in input_to_buffer:\n",
    "            self.deque.append(item)\n",
    "        \"\"\"\n",
    "        #Since we don't have many parallel environments, we can save the transition directly\n",
    "        self.deque.append(transition)\n",
    "\n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"sample from the buffer\"\"\"\n",
    "        samples = random.sample(self.deque, batchsize)\n",
    "\n",
    "        # transpose list of list\n",
    "        return transpose_list(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Episode 2\tScore: 0.073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_293/1260417356.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 256 elements not 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 115\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmaddpgRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 83\u001b[0m, in \u001b[0;36mmaddpgRunner\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):                \n\u001b[1;32m     82\u001b[0m         samples \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39msample(batchsize)\n\u001b[0;32m---> 83\u001b[0m         \u001b[43mmaddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     maddpg\u001b[38;5;241m.\u001b[39mupdate_targets() \u001b[38;5;66;03m#soft update the target network towards the actual networks\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#maddpg.update_targets() #soft update the target network towards the actual networks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 157\u001b[0m, in \u001b[0;36mMADDPG.update\u001b[0;34m(self, samples, agent_number)\u001b[0m\n\u001b[1;32m    147\u001b[0m agent\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m#critic loss = batch mean of (y- Q(s,a) from target network)^2 //perhaps it's Q(s,a) from critic network\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m#y = reward of this timestep + discount * Q(st+1,at+1c) from target network\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#While training the current agent, we use the actions and observations for all the agents to predict the next actions,  we use the full observation for all the agents along with the predicted next actions to predict the q values, but we use the reward and done values of the current agent being trained.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#print(len(next_obs)) #3\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m#print(next_obs[0].size()) #torch.Size([1000, 14])\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m target_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m#print(\"tensor actions size before cat:\")\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#print(len(target_actions)) #3\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m#print(target_actions.size())\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m#print(target_actions[0].size()) #torch.Size([1000, 2])\u001b[39;00m\n\u001b[1;32m    163\u001b[0m target_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(target_actions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# we do a cat of 3 tensors of size (1000,2) we'll obtai a tesor of size (1000,6)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 101\u001b[0m, in \u001b[0;36mMADDPG.target_act\u001b[0;34m(self, obs_all_agents, noise)\u001b[0m\n\u001b[1;32m     98\u001b[0m target_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharedActor:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m#this will be a tensor of tensor. We have to turn it into a list of tensors\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     target_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaddpg_agent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_all_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     target_actions \u001b[38;5;241m=\u001b[39m [tens \u001b[38;5;28;01mfor\u001b[39;00m tens \u001b[38;5;129;01min\u001b[39;00m target_actions]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_actions\n",
      "Cell \u001b[0;32mIn[13], line 117\u001b[0m, in \u001b[0;36mDDPGAgent.target_act\u001b[0;34m(self, obs, noise)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 117\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#if add_noise:            \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 80\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mx = self.fc1(state) \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mx = self.bn_input(x)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mreturn out\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(state))\n\u001b[0;32m---> 80\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 256 elements not 128"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "scores = []\n",
    "\n",
    "def maddpgRunner(n_episodes=3000, max_t=100, print_every=100):\n",
    "    \n",
    "    #Perhaps seeding\n",
    "    #...\n",
    "    \n",
    "    number_of_episodes = n_episodes\n",
    "    episode_length = max_t # 100 seems to be a value too high. 20, 30, 40, ...80,90 could be tested\n",
    "    batchsize = 128#256#128#20#100 #300 #500 # could try a greater number like 1000, 1500, 20000, ...\n",
    "    BUFFER_SIZE = int(5000*episode_length)#int(5000*episode_length) #10000\n",
    "    \n",
    "    # amplitude of OU noise\n",
    "    # this slowly decreases to 0\n",
    "    noise = 2\n",
    "    noise_reduction = 0.9999\n",
    "    \n",
    "    # how many episodes before update\n",
    "    episode_per_update = 6 #Could be changed with another number between 4 and 10#2 * parallel_envs\n",
    "    \n",
    "    #torch.set_num_threads(parallel_envs)\n",
    "    #env = envs.make_parallel_env(parallel_envs)\n",
    "    \n",
    "    # keep 5000 episodes worth of replay\n",
    "    #buffer = ReplayBuffer(int(5000*episode_length)) # could extend to 1000000\n",
    "    \n",
    "    action_size = 2 #not used in the replay buffer\n",
    "    random_seed = 2 #not used in the replay buffer\n",
    "    \n",
    "    buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "    \n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG(False, False, discount_factor=1) #there should be no discount factor for this project\n",
    "    #maddpg.createReplayBuffer(2, int(5000*episode_length), batchsize, 0)\n",
    "    \n",
    "    agent0_reward = []\n",
    "    agent1_reward = []\n",
    "    \n",
    "    scores_deque = deque(maxlen=print_every)                  # A queue to keep only the last 100 episodes' scores\n",
    "    \n",
    "    \n",
    "    for episode in range(1, number_of_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        score = np.zeros(num_agents)\n",
    "        #concStates = [] #concatenated states\n",
    "        #nextConcStates = [] #next concatenated states\n",
    "        \n",
    "        \n",
    "        for t in range(episode_length):\n",
    "            #Concatenate states to include information of oponent agent and get full information of the state of the game.\n",
    "            #It won't be possible to predict reliable actions without knowing the position of the oponent            \n",
    "            #concStates[0] = states[0] + states[1]\n",
    "            #concStates[1] = states[1] + states[0]\n",
    "            \n",
    "            #actions = maddpg.act(concStates,noise=noise)\n",
    "            actions = maddpg.act(states,noise=noise)\n",
    "            noise *= noise_reduction\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment            \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            score += env_info.rewards                         # update the score (for each agent)\n",
    "            #nextConcStates[0] = next_states[0] + next_states[1]\n",
    "            #nextConcStates[1] = next_states[1] + next_states[0]\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            \n",
    "            #WE SHOULD SAVE WITHOUT ZIPPING\n",
    "            #for state, action, reward, next_state, done in zip(concStates, actions, rewards, nextConcStates, dones):\n",
    "                # Save experience / reward for all the agents\n",
    "                #self.memory.add(state, action, reward, next_state, done)\n",
    "            transition = (states, actions, rewards, next_states, dones)\n",
    "            buffer.push(list(transition))\n",
    "            \n",
    "            if len(buffer) > batchsize and (t%20) == 0:\n",
    "                #train the 2 agents\n",
    "                for _ in range(4) :\n",
    "                    for a_i in range(2):                \n",
    "                        samples = buffer.sample(batchsize)\n",
    "                        maddpg.update(samples, a_i)\n",
    "                    maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                #maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                \n",
    "            \n",
    "        score = max(score)#score.mean()                                  # Get the mean score of the episode (over all agents)\n",
    "        scores_deque.append(score)                            # Store the score in the queue\n",
    "        scores.append(score)                                  # Store the score in the list (for plotting)\n",
    "        \n",
    "        \n",
    "        # Print out the max score per episode\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(episode, score), end=\"\")\n",
    "\n",
    "        mean_sc = np.mean(scores_deque)                       # Compute the mean score over the last 100 episodes\n",
    "        if episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, mean_sc))\n",
    "            \n",
    "        if len(scores_deque) == 100 and mean_sc >= 0.5 :\n",
    "            print('\\rEnvironment solved in {} episodes, mean score: {:.2f}'.format(i_episode, mean_sc))\n",
    "            \n",
    "            agi = 0\n",
    "            \n",
    "            for ddpgAgent in self.maddpg:\n",
    "                torch.save(ddpgAgent.actor.state_dict(), 'checkpoint_actor{}.pth'.format(agi))\n",
    "                torch.save(ddpgAgent.critic.state_dict(), 'checkpoint_critic{}.pth'.format(agi))\n",
    "                agi = agi + 1\n",
    "            break\n",
    "                \n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"start\")\n",
    "scores = maddpgRunner()\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
